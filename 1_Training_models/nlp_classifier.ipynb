{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Clean DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to s3 instance\n",
    "import boto3\n",
    "YOUR_ACCESS_KEY = \n",
    "YOUR_SECRET_KEY = \n",
    "\n",
    "session = boto3.Session(aws_access_key_id= YOUR_ACCESS_KEY, \n",
    "                        aws_secret_access_key= YOUR_SECRET_KEY)\n",
    "\n",
    "s3 = session.resource(\"s3\")\n",
    "client = session.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full DS\n",
    "obj = s3.Object('jedha-fake-reviews-project', \"datasets/full_dataset.csv\")\n",
    "dataset = pd.read_csv(io.BytesIO(obj.get()['Body'].read()), low_memory = False, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____________________________________________________________________\n",
    "######### Cleaning the dataset and adding new columns #########\n",
    "#_____________________________________________________________________\n",
    "\n",
    "#we drop rows in which restaurant infos are not available (miss scraped)\n",
    "dataset = dataset.dropna(subset = ['restaurant_average_rating', 'restaurant_reviews_count', 'restaurant_expensiveness', 'restaurant_name'])\n",
    "\n",
    "#adding a column with the length of the text review\n",
    "dataset['text_length'] = dataset['text_review'].apply(lambda x : len(x))\n",
    "\n",
    "#_____________________________________________________________________\n",
    "######### Fixing existing columns values and types #########\n",
    "#_____________________________________________________________________\n",
    "\n",
    "#for the user_total_image_posted column, if user_total_image_posted is NA it means there is there's no image\n",
    "    # so we set the value to 0\n",
    "dataset.loc[dataset['user_total_image_posted'].isna(), 'user_total_image_posted'] = 0\n",
    "\n",
    "#for the date column,  there is some miss scraps that we want to fix\n",
    "    # a correct data must have a length of 10 , if it is smaller than 10 it's becasue we scrapped the number of images of the user instead\n",
    "    # we may have to scrap again those lines to fix it\n",
    "    # we keep only the rows where the date is correct \n",
    "mask_not_date = dataset['date'].apply(lambda x: len(x)) < 10\n",
    "dataset = dataset.loc[mask_not_date == False, :]\n",
    "    # if te length is greater than 10 is it is beacause we scraped the date + somme additional words ('Avis mis à jour') so we will keep only the part with the date\n",
    "mask_date_to_fix = dataset['date'].apply(lambda x: len(x)) > 10\n",
    "dataset.loc[mask_date_to_fix, 'date' ] = dataset.loc[mask_date_to_fix, 'date' ].str.split('\\n').str[0]\n",
    "    #finally we can convert the date column to a datetime format\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "\n",
    "#for the photos_for_review column, \n",
    "    # value -1 is in fact 0 (no photos found by the scraper)\n",
    "dataset.loc[dataset['photos_for_review'] == '-1.0', 'photos_for_review' ] = '0'\n",
    "    # value L is in fact 0 (no photos found by the scraper but scraped the first letter of \"L'avis du jour\" which happens when the reviews was updated by the user)\n",
    "dataset.loc[dataset['photos_for_review'] == 'L', 'photos_for_review' ] = '0'\n",
    "    # finally we can convert the photos_for_review column to an int format\n",
    "dataset['photos_for_review'] = dataset['photos_for_review'].astype('int')\n",
    "\n",
    "#for the photos_for_review column, \n",
    "    # when there's no info about the expensiveness we set it to -1\n",
    "dataset.loc[dataset['restaurant_expensiveness'] == 'N/C', 'restaurant_expensiveness']  = -1\n",
    "    # we can convert the restaurant_expensiveness column to an int format\n",
    "dataset['restaurant_expensiveness'] = dataset['restaurant_expensiveness'].astype('int')\n",
    "\n",
    "# change is real review for is fake review as it's better for sklearn \n",
    "dataset[\"is_fake_review\"] = dataset[\"is_real_review\"].apply(lambda x: '1' if x == 0 else '0')\n",
    "dataset[\"is_fake_review\"] = dataset[\"is_fake_review\"].astype(int)\n",
    "dataset = dataset.drop(columns=\"is_real_review\")\n",
    "\n",
    "# reset index \n",
    "dataset = dataset.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_reviews = dataset.loc[dataset['language'] =='fr',['text_review', 'is_fake_review']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_review</th>\n",
       "      <th>is_fake_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bon retour !\\nJe suis revenue dans ce resto ap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A optimiser...\\nCuisine très traditionnelle da...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brasserie chic\\nUne brasserie authentiquement ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tres bien\\nPetit diner entre amis. Les plats e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Un bistrot bien sympathique\\nNous avons mangé ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87046</th>\n",
       "      <td>Du choix, un service extrêmement rapide, le re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87047</th>\n",
       "      <td>Vraiment un des meilleur kebab du coin, servic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87048</th>\n",
       "      <td>Très déçu!!!\\nCe soir j'ai eu envie de manger ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87049</th>\n",
       "      <td>J'y vais depuis le début mais j'avoue qu'avec ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87050</th>\n",
       "      <td>Meilleur grec dans les environs + personnel au...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87051 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_review  is_fake_review\n",
       "0      Bon retour !\\nJe suis revenue dans ce resto ap...               0\n",
       "1      A optimiser...\\nCuisine très traditionnelle da...               0\n",
       "2      Brasserie chic\\nUne brasserie authentiquement ...               0\n",
       "3      Tres bien\\nPetit diner entre amis. Les plats e...               0\n",
       "4      Un bistrot bien sympathique\\nNous avons mangé ...               0\n",
       "...                                                  ...             ...\n",
       "87046  Du choix, un service extrêmement rapide, le re...               1\n",
       "87047  Vraiment un des meilleur kebab du coin, servic...               1\n",
       "87048  Très déçu!!!\\nCe soir j'ai eu envie de manger ...               1\n",
       "87049  J'y vais depuis le début mais j'avoue qu'avec ...               1\n",
       "87050  Meilleur grec dans les environs + personnel au...               1\n",
       "\n",
       "[87051 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = french_reviews.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip\n",
    "data[\"text_review_clean\"] = data[\"text_review\"].str.strip()\n",
    "\n",
    "#lower\n",
    "data[\"text_review_clean\"] = data[\"text_review_clean\"].str.lower()\n",
    "\n",
    "data[\"text_review_clean\"] = data[\"text_review_clean\"].str.replace('\\n', ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text_review_clean\"] = data[\"text_review_clean\"].str.replace(r\"<[a-z/]+>\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text_review_clean\"] = data[\"text_review_clean\"].str.replace(r\"[^A-zÀ-ÿ0-9' ]+\", \" \").astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing, lemmatizing and deleteing stopwords from doc with Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89422\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# first let's find the count of all words and return them in the form of dict items\n",
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(' '.join(data[\"text_review_clean\"]).split()).items() #\n",
    "print(len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with all words and their count\n",
    "word_count = pd.DataFrame({'word': [item[0] for item in list(word_count)], \n",
    "             'count' : [item[1] for item in list (word_count)]})\n",
    "\n",
    "# format\n",
    "word_count = word_count.sort_values('count', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>de</td>\n",
       "      <td>258291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>et</td>\n",
       "      <td>219008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>le</td>\n",
       "      <td>189740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>la</td>\n",
       "      <td>169772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>un</td>\n",
       "      <td>144444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>porc</td>\n",
       "      <td>2033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3607</th>\n",
       "      <td>bière</td>\n",
       "      <td>2029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>vue</td>\n",
       "      <td>2028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>délice</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>30</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>435 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word   count\n",
       "12        de  258291\n",
       "29        et  219008\n",
       "17        le  189740\n",
       "21        la  169772\n",
       "60        un  144444\n",
       "...      ...     ...\n",
       "1921    porc    2033\n",
       "3607   bière    2029\n",
       "1013     vue    2028\n",
       "641   délice    2016\n",
       "628       30    2010\n",
       "\n",
       "[435 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take all words that occur more than 500 times\n",
    "commonwords = word_count.loc[word_count[\"count\"]>=2000, :]\n",
    "commonwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nlp instance\n",
    "nlp =  fr_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-6bd5b2b24704>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  commonwords[\"word\"] = commonwords[\"word\"].apply(lambda x: nlp(x))\n",
      "<ipython-input-16-6bd5b2b24704>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  commonwords[\"word\"] = commonwords[\"word\"].apply(lambda x: [token.lemma_ for token in x])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[de]</td>\n",
       "      <td>258291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[et]</td>\n",
       "      <td>219008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[le]</td>\n",
       "      <td>189740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[le]</td>\n",
       "      <td>169772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>[un]</td>\n",
       "      <td>144444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word   count\n",
       "12  [de]  258291\n",
       "29  [et]  219008\n",
       "17  [le]  189740\n",
       "21  [le]  169772\n",
       "60  [un]  144444"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# lemmatize common words \n",
    "commonwords[\"word\"] = commonwords[\"word\"].apply(lambda x: nlp(x))\n",
    "commonwords[\"word\"] = commonwords[\"word\"].apply(lambda x: [token.lemma_ for token in x])\n",
    "commonwords.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-3fbb5fd649c6>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  commonwords[\"word\"] = commonwords[\"word\"].str.join(\"\")\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "commonwords[\"word\"] = commonwords[\"word\"].str.join(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12          de\n",
       "29          et\n",
       "17          le\n",
       "21          le\n",
       "60          un\n",
       "         ...  \n",
       "1921      porc\n",
       "3607     bière\n",
       "1013       vue\n",
       "641     délice\n",
       "628         30\n",
       "Name: word, Length: 435, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make list\n",
    "common_words = commonwords.word\n",
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "852\n"
     ]
    }
   ],
   "source": [
    "# append to stopwords \n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "print(len(STOP_WORDS))\n",
    "STOP_WORDS_MAX = STOP_WORDS.union(common_words)\n",
    "\n",
    "# also add the lemmatizer for pronouns as we won't need them\n",
    "STOP_WORDS_MAX.add(\"-PRON-\")\n",
    "print(len(STOP_WORDS_MAX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  apply nlp to transform into doc\n",
    "data[\"clean_tokens\"] = data[\"text_review_clean\"].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_review</th>\n",
       "      <th>is_fake_review</th>\n",
       "      <th>text_review_clean</th>\n",
       "      <th>clean_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bon retour !\\nJe suis revenue dans ce resto ap...</td>\n",
       "      <td>0</td>\n",
       "      <td>bon retour   je suis revenue dans ce resto apr...</td>\n",
       "      <td>(bon, retour,   , je, suis, revenue, dans, ce,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A optimiser...\\nCuisine très traditionnelle da...</td>\n",
       "      <td>0</td>\n",
       "      <td>a optimiser  cuisine très traditionnelle dans ...</td>\n",
       "      <td>(a, optimiser,  , cuisine, très, traditionnell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brasserie chic\\nUne brasserie authentiquement ...</td>\n",
       "      <td>0</td>\n",
       "      <td>brasserie chic une brasserie authentiquement p...</td>\n",
       "      <td>(brasserie, chic, une, brasserie, authentiquem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tres bien\\nPetit diner entre amis. Les plats e...</td>\n",
       "      <td>0</td>\n",
       "      <td>tres bien petit diner entre amis  les plats et...</td>\n",
       "      <td>(tres, bien, petit, diner, entre, amis,  , les...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Un bistrot bien sympathique\\nNous avons mangé ...</td>\n",
       "      <td>0</td>\n",
       "      <td>un bistrot bien sympathique nous avons mangé e...</td>\n",
       "      <td>(un, bistrot, bien, sympathique, nous, avons, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_review  is_fake_review  \\\n",
       "0  Bon retour !\\nJe suis revenue dans ce resto ap...               0   \n",
       "1  A optimiser...\\nCuisine très traditionnelle da...               0   \n",
       "2  Brasserie chic\\nUne brasserie authentiquement ...               0   \n",
       "3  Tres bien\\nPetit diner entre amis. Les plats e...               0   \n",
       "4  Un bistrot bien sympathique\\nNous avons mangé ...               0   \n",
       "\n",
       "                                   text_review_clean  \\\n",
       "0  bon retour   je suis revenue dans ce resto apr...   \n",
       "1  a optimiser  cuisine très traditionnelle dans ...   \n",
       "2  brasserie chic une brasserie authentiquement p...   \n",
       "3  tres bien petit diner entre amis  les plats et...   \n",
       "4  un bistrot bien sympathique nous avons mangé e...   \n",
       "\n",
       "                                        clean_tokens  \n",
       "0  (bon, retour,   , je, suis, revenue, dans, ce,...  \n",
       "1  (a, optimiser,  , cuisine, très, traditionnell...  \n",
       "2  (brasserie, chic, une, brasserie, authentiquem...  \n",
       "3  (tres, bien, petit, diner, entre, amis,  , les...  \n",
       "4  (un, bistrot, bien, sympathique, nous, avons, ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_review</th>\n",
       "      <th>is_fake_review</th>\n",
       "      <th>text_review_clean</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>clean_tokens_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bon retour !\\nJe suis revenue dans ce resto ap...</td>\n",
       "      <td>0</td>\n",
       "      <td>bon retour   je suis revenue dans ce resto apr...</td>\n",
       "      <td>(bon, retour,   , je, suis, revenue, dans, ce,...</td>\n",
       "      <td>[  , revenir, long, absence, an,  ,   , change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A optimiser...\\nCuisine très traditionnelle da...</td>\n",
       "      <td>0</td>\n",
       "      <td>a optimiser  cuisine très traditionnelle dans ...</td>\n",
       "      <td>(a, optimiser,  , cuisine, très, traditionnell...</td>\n",
       "      <td>[optimiser,  , traditionnel,  , soigner,  ,  ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brasserie chic\\nUne brasserie authentiquement ...</td>\n",
       "      <td>0</td>\n",
       "      <td>brasserie chic une brasserie authentiquement p...</td>\n",
       "      <td>(brasserie, chic, une, brasserie, authentiquem...</td>\n",
       "      <td>[chic, authentiquement, parisien, pouce, raffi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tres bien\\nPetit diner entre amis. Les plats e...</td>\n",
       "      <td>0</td>\n",
       "      <td>tres bien petit diner entre amis  les plats et...</td>\n",
       "      <td>(tres, bien, petit, diner, entre, amis,  , les...</td>\n",
       "      <td>[diner,  ,  , tarte, framboise, exquis,  ,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Un bistrot bien sympathique\\nNous avons mangé ...</td>\n",
       "      <td>0</td>\n",
       "      <td>un bistrot bien sympathique nous avons mangé e...</td>\n",
       "      <td>(un, bistrot, bien, sympathique, nous, avons, ...</td>\n",
       "      <td>[famille, type,  ,  , fort, aimable,  , additi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_review  is_fake_review  \\\n",
       "0  Bon retour !\\nJe suis revenue dans ce resto ap...               0   \n",
       "1  A optimiser...\\nCuisine très traditionnelle da...               0   \n",
       "2  Brasserie chic\\nUne brasserie authentiquement ...               0   \n",
       "3  Tres bien\\nPetit diner entre amis. Les plats e...               0   \n",
       "4  Un bistrot bien sympathique\\nNous avons mangé ...               0   \n",
       "\n",
       "                                   text_review_clean  \\\n",
       "0  bon retour   je suis revenue dans ce resto apr...   \n",
       "1  a optimiser  cuisine très traditionnelle dans ...   \n",
       "2  brasserie chic une brasserie authentiquement p...   \n",
       "3  tres bien petit diner entre amis  les plats et...   \n",
       "4  un bistrot bien sympathique nous avons mangé e...   \n",
       "\n",
       "                                        clean_tokens  \\\n",
       "0  (bon, retour,   , je, suis, revenue, dans, ce,...   \n",
       "1  (a, optimiser,  , cuisine, très, traditionnell...   \n",
       "2  (brasserie, chic, une, brasserie, authentiquem...   \n",
       "3  (tres, bien, petit, diner, entre, amis,  , les...   \n",
       "4  (un, bistrot, bien, sympathique, nous, avons, ...   \n",
       "\n",
       "                             clean_tokens_lemmatized  \n",
       "0  [  , revenir, long, absence, an,  ,   , change...  \n",
       "1  [optimiser,  , traditionnel,  , soigner,  ,  ,...  \n",
       "2  [chic, authentiquement, parisien, pouce, raffi...  \n",
       "3  [diner,  ,  , tarte, framboise, exquis,  ,  , ...  \n",
       "4  [famille, type,  ,  , fort, aimable,  , additi...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize each token and remove stop words --> could be done in two steps but we do it in one\n",
    "data['clean_tokens_lemmatized'] = data['clean_tokens'].apply(lambda doc: [token.lemma_ for token in doc if token.lemma_ not in STOP_WORDS_MAX])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### join all of them into new df column\n",
    "# method 1\n",
    "data[\"clean_review\"] = data[\"clean_tokens_lemmatized\"].str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87051, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_review</th>\n",
       "      <th>is_fake_review</th>\n",
       "      <th>text_review_clean</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>clean_tokens_lemmatized</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68648</th>\n",
       "      <td>Pour les gourmants !\\nAvec mon ami, nous somme...</td>\n",
       "      <td>0</td>\n",
       "      <td>pour les gourmants   avec mon ami  nous sommes...</td>\n",
       "      <td>(pour, les, gourmants,   , avec, mon, ami,  , ...</td>\n",
       "      <td>[gourmant,   ,  , hasard,  , fête,  , d, ',  ,...</td>\n",
       "      <td>gourmant      hasard   fête   d '   charme lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7066</th>\n",
       "      <td>Un adresse incontournable\\nBien que venu décou...</td>\n",
       "      <td>0</td>\n",
       "      <td>un adresse incontournable bien que venu découv...</td>\n",
       "      <td>(un, adresse, incontournable, bien, que, venu,...</td>\n",
       "      <td>[incontournable, péruvien, typique, payer, min...</td>\n",
       "      <td>incontournable péruvien typique payer mine vei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14444</th>\n",
       "      <td>Très bien, déjà deux fois qui j'y vais.\\nUne f...</td>\n",
       "      <td>0</td>\n",
       "      <td>très bien  déjà deux fois qui j'y vais  une fo...</td>\n",
       "      <td>(très, bien,  , déjà, deux, fois, qui, j', y, ...</td>\n",
       "      <td>[ ,  , week, end, proposer,  , quantitée, géné...</td>\n",
       "      <td>week end proposer   quantitée généreux wee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45240</th>\n",
       "      <td>Comme dans un paradis\\nJe suis très contente d...</td>\n",
       "      <td>0</td>\n",
       "      <td>comme dans un paradis je suis très contente de...</td>\n",
       "      <td>(comme, dans, un, paradis, je, suis, très, con...</td>\n",
       "      <td>[paradis, content, connaître,  , zen, romantiq...</td>\n",
       "      <td>paradis content connaître   zen romantique   v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1 heure d'attente pour le plat principal\\nPas ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1 heure d'attente pour le plat principal pas m...</td>\n",
       "      <td>(1, heure, d', attente, pour, le, plat, princi...</td>\n",
       "      <td>[heure, attente, principal,  , lamentable,  , ...</td>\n",
       "      <td>heure attente principal   lamentable   1h prin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_review  is_fake_review  \\\n",
       "68648  Pour les gourmants !\\nAvec mon ami, nous somme...               0   \n",
       "7066   Un adresse incontournable\\nBien que venu décou...               0   \n",
       "14444  Très bien, déjà deux fois qui j'y vais.\\nUne f...               0   \n",
       "45240  Comme dans un paradis\\nJe suis très contente d...               0   \n",
       "51     1 heure d'attente pour le plat principal\\nPas ...               0   \n",
       "\n",
       "                                       text_review_clean  \\\n",
       "68648  pour les gourmants   avec mon ami  nous sommes...   \n",
       "7066   un adresse incontournable bien que venu découv...   \n",
       "14444  très bien  déjà deux fois qui j'y vais  une fo...   \n",
       "45240  comme dans un paradis je suis très contente de...   \n",
       "51     1 heure d'attente pour le plat principal pas m...   \n",
       "\n",
       "                                            clean_tokens  \\\n",
       "68648  (pour, les, gourmants,   , avec, mon, ami,  , ...   \n",
       "7066   (un, adresse, incontournable, bien, que, venu,...   \n",
       "14444  (très, bien,  , déjà, deux, fois, qui, j', y, ...   \n",
       "45240  (comme, dans, un, paradis, je, suis, très, con...   \n",
       "51     (1, heure, d', attente, pour, le, plat, princi...   \n",
       "\n",
       "                                 clean_tokens_lemmatized  \\\n",
       "68648  [gourmant,   ,  , hasard,  , fête,  , d, ',  ,...   \n",
       "7066   [incontournable, péruvien, typique, payer, min...   \n",
       "14444  [ ,  , week, end, proposer,  , quantitée, géné...   \n",
       "45240  [paradis, content, connaître,  , zen, romantiq...   \n",
       "51     [heure, attente, principal,  , lamentable,  , ...   \n",
       "\n",
       "                                            clean_review  \n",
       "68648  gourmant      hasard   fête   d '   charme lov...  \n",
       "7066   incontournable péruvien typique payer mine vei...  \n",
       "14444      week end proposer   quantitée généreux wee...  \n",
       "45240  paradis content connaître   zen romantique   v...  \n",
       "51     heure attente principal   lamentable   1h prin...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(data.shape)\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a TFIDF Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply vectorizer to the review column\n",
    "vectorizer = TfidfVectorizer(smooth_idf=True, min_df=200)\n",
    "X = vectorizer.fit_transform(data['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1752"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87051, 1752)\n"
     ]
    }
   ],
   "source": [
    "# transform this sparse matrix into a numpy array \n",
    "X_dense = X.toarray()\n",
    "print(X_dense.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put the matrix into a DF with the feature name (ie word) as column title and the document number as ID\n",
    "# this is easily doable because the get_feature_names method of vectorizer returns the feature names \n",
    "# with the same index as their values in the X_dense matrix\n",
    "X_df = pd.DataFrame(X_dense, \n",
    "             columns=[x for x in vectorizer.get_feature_names()], \n",
    "             index=[\"review_{}\".format(i) for i in range (1,87052)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>13</th>\n",
       "      <th>13h</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>19h</th>\n",
       "      <th>...</th>\n",
       "      <th>étonnant</th>\n",
       "      <th>étonner</th>\n",
       "      <th>étrange</th>\n",
       "      <th>étranger</th>\n",
       "      <th>étroit</th>\n",
       "      <th>étudiant</th>\n",
       "      <th>évidemment</th>\n",
       "      <th>évident</th>\n",
       "      <th>éviter</th>\n",
       "      <th>île</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>review_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_87047</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_87048</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_87049</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_87050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_87051</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87051 rows × 1752 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              100   11   13  13h   14   16   17   18   19  19h  ...  étonnant  \\\n",
       "review_1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "review_2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "review_3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "review_4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "review_5      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "...           ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   \n",
       "review_87047  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "review_87048  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "review_87049  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "review_87050  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "review_87051  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "\n",
       "              étonner  étrange  étranger  étroit  étudiant  évidemment  \\\n",
       "review_1          0.0      0.0       0.0     0.0       0.0         0.0   \n",
       "review_2          0.0      0.0       0.0     0.0       0.0         0.0   \n",
       "review_3          0.0      0.0       0.0     0.0       0.0         0.0   \n",
       "review_4          0.0      0.0       0.0     0.0       0.0         0.0   \n",
       "review_5          0.0      0.0       0.0     0.0       0.0         0.0   \n",
       "...               ...      ...       ...     ...       ...         ...   \n",
       "review_87047      0.0      0.0       0.0     0.0       0.0         0.0   \n",
       "review_87048      0.0      0.0       0.0     0.0       0.0         0.0   \n",
       "review_87049      0.0      0.0       0.0     0.0       0.0         0.0   \n",
       "review_87050      0.0      0.0       0.0     0.0       0.0         0.0   \n",
       "review_87051      0.0      0.0       0.0     0.0       0.0         0.0   \n",
       "\n",
       "              évident  éviter  île  \n",
       "review_1          0.0     0.0  0.0  \n",
       "review_2          0.0     0.0  0.0  \n",
       "review_3          0.0     0.0  0.0  \n",
       "review_4          0.0     0.0  0.0  \n",
       "review_5          0.0     0.0  0.0  \n",
       "...               ...     ...  ...  \n",
       "review_87047      0.0     0.0  0.0  \n",
       "review_87048      0.0     0.0  0.0  \n",
       "review_87049      0.0     0.0  0.0  \n",
       "review_87050      0.0     0.0  0.0  \n",
       "review_87051      0.0     0.0  0.0  \n",
       "\n",
       "[87051 rows x 1752 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from sklearn\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set it to 12 different topics \n",
    "svd = TruncatedSVD(n_components= 70)\n",
    "\n",
    "# fit to our matrix --> last two columns are those with the previous cluster_values\n",
    "lsa = svd.fit_transform(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1557903741267197\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_61</th>\n",
       "      <th>topic_62</th>\n",
       "      <th>topic_63</th>\n",
       "      <th>topic_64</th>\n",
       "      <th>topic_65</th>\n",
       "      <th>topic_66</th>\n",
       "      <th>topic_67</th>\n",
       "      <th>topic_68</th>\n",
       "      <th>topic_69</th>\n",
       "      <th>topic_70</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.182937</td>\n",
       "      <td>-0.037876</td>\n",
       "      <td>-0.021556</td>\n",
       "      <td>-0.031425</td>\n",
       "      <td>-0.015129</td>\n",
       "      <td>-0.019524</td>\n",
       "      <td>0.037902</td>\n",
       "      <td>0.042148</td>\n",
       "      <td>-0.059362</td>\n",
       "      <td>0.178819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.080641</td>\n",
       "      <td>-0.141749</td>\n",
       "      <td>0.005719</td>\n",
       "      <td>-0.024154</td>\n",
       "      <td>-0.010394</td>\n",
       "      <td>0.040280</td>\n",
       "      <td>0.048726</td>\n",
       "      <td>-0.035746</td>\n",
       "      <td>-0.012122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081317</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.003718</td>\n",
       "      <td>0.037146</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.012801</td>\n",
       "      <td>-0.033361</td>\n",
       "      <td>0.013321</td>\n",
       "      <td>-0.000854</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011674</td>\n",
       "      <td>0.031722</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.017245</td>\n",
       "      <td>-0.103015</td>\n",
       "      <td>0.037871</td>\n",
       "      <td>-0.067699</td>\n",
       "      <td>0.031082</td>\n",
       "      <td>0.021013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072941</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.032298</td>\n",
       "      <td>0.029774</td>\n",
       "      <td>-0.022186</td>\n",
       "      <td>-0.071312</td>\n",
       "      <td>0.007261</td>\n",
       "      <td>-0.030092</td>\n",
       "      <td>0.052803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086015</td>\n",
       "      <td>0.053606</td>\n",
       "      <td>-0.079024</td>\n",
       "      <td>-0.144618</td>\n",
       "      <td>0.064571</td>\n",
       "      <td>-0.039876</td>\n",
       "      <td>0.137298</td>\n",
       "      <td>0.015482</td>\n",
       "      <td>-0.097854</td>\n",
       "      <td>0.007026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.094846</td>\n",
       "      <td>-0.000364</td>\n",
       "      <td>0.005569</td>\n",
       "      <td>0.033342</td>\n",
       "      <td>0.028184</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>-0.047884</td>\n",
       "      <td>0.041983</td>\n",
       "      <td>-0.050951</td>\n",
       "      <td>0.016491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096816</td>\n",
       "      <td>-0.023996</td>\n",
       "      <td>0.004972</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>0.060896</td>\n",
       "      <td>-0.045629</td>\n",
       "      <td>0.019040</td>\n",
       "      <td>0.082028</td>\n",
       "      <td>0.021663</td>\n",
       "      <td>0.085922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.114763</td>\n",
       "      <td>-0.012122</td>\n",
       "      <td>-0.005755</td>\n",
       "      <td>-0.001325</td>\n",
       "      <td>-0.017749</td>\n",
       "      <td>-0.008861</td>\n",
       "      <td>0.011973</td>\n",
       "      <td>0.004776</td>\n",
       "      <td>0.010265</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073497</td>\n",
       "      <td>-0.016962</td>\n",
       "      <td>-0.020629</td>\n",
       "      <td>0.054399</td>\n",
       "      <td>-0.023851</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.012216</td>\n",
       "      <td>-0.115549</td>\n",
       "      <td>0.082958</td>\n",
       "      <td>-0.088175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_1   topic_2   topic_3   topic_4   topic_5   topic_6   topic_7  \\\n",
       "0  0.182937 -0.037876 -0.021556 -0.031425 -0.015129 -0.019524  0.037902   \n",
       "1  0.081317 -0.005818 -0.003718  0.037146  0.031008  0.012801 -0.033361   \n",
       "2  0.072941 -0.001708  0.006482  0.032298  0.029774 -0.022186 -0.071312   \n",
       "3  0.094846 -0.000364  0.005569  0.033342  0.028184  0.005248 -0.047884   \n",
       "4  0.114763 -0.012122 -0.005755 -0.001325 -0.017749 -0.008861  0.011973   \n",
       "\n",
       "    topic_8   topic_9  topic_10  ...  topic_61  topic_62  topic_63  topic_64  \\\n",
       "0  0.042148 -0.059362  0.178819  ...  0.000717  0.080641 -0.141749  0.005719   \n",
       "1  0.013321 -0.000854  0.008429  ... -0.011674  0.031722  0.004676  0.002275   \n",
       "2  0.007261 -0.030092  0.052803  ... -0.086015  0.053606 -0.079024 -0.144618   \n",
       "3  0.041983 -0.050951  0.016491  ...  0.096816 -0.023996  0.004972  0.010744   \n",
       "4  0.004776  0.010265  0.024023  ...  0.073497 -0.016962 -0.020629  0.054399   \n",
       "\n",
       "   topic_65  topic_66  topic_67  topic_68  topic_69  topic_70  \n",
       "0 -0.024154 -0.010394  0.040280  0.048726 -0.035746 -0.012122  \n",
       "1  0.017245 -0.103015  0.037871 -0.067699  0.031082  0.021013  \n",
       "2  0.064571 -0.039876  0.137298  0.015482 -0.097854  0.007026  \n",
       "3  0.060896 -0.045629  0.019040  0.082028  0.021663  0.085922  \n",
       "4 -0.023851  0.007332  0.012216 -0.115549  0.082958 -0.088175  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_encoded_df = pd.DataFrame(lsa, columns = [\"topic_{}\".format(i) \\\n",
    "                                                for i in range(1,(lsa.shape[1]+1))]\\\n",
    "                               )\n",
    "topic_encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data For Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cl = topic_encoded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"len_review\"] = data[\"text_review\"].apply(lambda x : len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['upper_word_count'] = data['text_review'].apply(lambda x : sum(map(str.isupper, x.split())) )\n",
    "data['upper_word_count'] = pd.qcut(data['upper_word_count'].rank(method = 'first'), 3, labels = ['low', 'mid', 'high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['exclam_count'] = data['text_review'].apply(lambda x : len(''.join(ch for ch in x if ch =='!')))\n",
    "data['exclam_count'] = pd.qcut(data['exclam_count'].rank(method = 'first'), 3, labels = ['low', 'high', 'very_high'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exclam_count  is_fake_review\n",
       "low           0                 1.000000\n",
       "high          0                 0.674605\n",
       "              1                 0.325395\n",
       "very_high     0                 0.848675\n",
       "              1                 0.151325\n",
       "Name: is_fake_review, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('exclam_count')[\"is_fake_review\"].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append review length \n",
    "data_cl[\"len_review\"] = list(data.len_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append fake review class\n",
    "data_cl[\"is_fake_review\"] = list(data[\"is_fake_review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append uppercase\n",
    "data_cl[\"upper_word_count\"] = list(data[\"upper_word_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append exclam\n",
    "data_cl[\"exclam_count\"] = list(data[\"exclam_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_65</th>\n",
       "      <th>topic_66</th>\n",
       "      <th>topic_67</th>\n",
       "      <th>topic_68</th>\n",
       "      <th>topic_69</th>\n",
       "      <th>topic_70</th>\n",
       "      <th>len_review</th>\n",
       "      <th>is_fake_review</th>\n",
       "      <th>upper_word_count</th>\n",
       "      <th>exclam_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.182937</td>\n",
       "      <td>-0.037876</td>\n",
       "      <td>-0.021556</td>\n",
       "      <td>-0.031425</td>\n",
       "      <td>-0.015129</td>\n",
       "      <td>-0.019524</td>\n",
       "      <td>0.037902</td>\n",
       "      <td>0.042148</td>\n",
       "      <td>-0.059362</td>\n",
       "      <td>0.178819</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024154</td>\n",
       "      <td>-0.010394</td>\n",
       "      <td>0.040280</td>\n",
       "      <td>0.048726</td>\n",
       "      <td>-0.035746</td>\n",
       "      <td>-0.012122</td>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081317</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.003718</td>\n",
       "      <td>0.037146</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.012801</td>\n",
       "      <td>-0.033361</td>\n",
       "      <td>0.013321</td>\n",
       "      <td>-0.000854</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017245</td>\n",
       "      <td>-0.103015</td>\n",
       "      <td>0.037871</td>\n",
       "      <td>-0.067699</td>\n",
       "      <td>0.031082</td>\n",
       "      <td>0.021013</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072941</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.032298</td>\n",
       "      <td>0.029774</td>\n",
       "      <td>-0.022186</td>\n",
       "      <td>-0.071312</td>\n",
       "      <td>0.007261</td>\n",
       "      <td>-0.030092</td>\n",
       "      <td>0.052803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064571</td>\n",
       "      <td>-0.039876</td>\n",
       "      <td>0.137298</td>\n",
       "      <td>0.015482</td>\n",
       "      <td>-0.097854</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>323</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.094846</td>\n",
       "      <td>-0.000364</td>\n",
       "      <td>0.005569</td>\n",
       "      <td>0.033342</td>\n",
       "      <td>0.028184</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>-0.047884</td>\n",
       "      <td>0.041983</td>\n",
       "      <td>-0.050951</td>\n",
       "      <td>0.016491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060896</td>\n",
       "      <td>-0.045629</td>\n",
       "      <td>0.019040</td>\n",
       "      <td>0.082028</td>\n",
       "      <td>0.021663</td>\n",
       "      <td>0.085922</td>\n",
       "      <td>247</td>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.114763</td>\n",
       "      <td>-0.012122</td>\n",
       "      <td>-0.005755</td>\n",
       "      <td>-0.001325</td>\n",
       "      <td>-0.017749</td>\n",
       "      <td>-0.008861</td>\n",
       "      <td>0.011973</td>\n",
       "      <td>0.004776</td>\n",
       "      <td>0.010265</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023851</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.012216</td>\n",
       "      <td>-0.115549</td>\n",
       "      <td>0.082958</td>\n",
       "      <td>-0.088175</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87046</th>\n",
       "      <td>0.056939</td>\n",
       "      <td>-0.014500</td>\n",
       "      <td>-0.009422</td>\n",
       "      <td>-0.012278</td>\n",
       "      <td>-0.012643</td>\n",
       "      <td>-0.000716</td>\n",
       "      <td>-0.001827</td>\n",
       "      <td>-0.006371</td>\n",
       "      <td>-0.001868</td>\n",
       "      <td>-0.014049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061815</td>\n",
       "      <td>-0.002758</td>\n",
       "      <td>-0.020445</td>\n",
       "      <td>0.017416</td>\n",
       "      <td>0.016560</td>\n",
       "      <td>-0.022601</td>\n",
       "      <td>195</td>\n",
       "      <td>1</td>\n",
       "      <td>high</td>\n",
       "      <td>very_high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87047</th>\n",
       "      <td>0.015847</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.005971</td>\n",
       "      <td>-0.006066</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>-0.003124</td>\n",
       "      <td>-0.012159</td>\n",
       "      <td>0.003439</td>\n",
       "      <td>-0.002020</td>\n",
       "      <td>-0.035379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034951</td>\n",
       "      <td>-0.005332</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.042685</td>\n",
       "      <td>0.046822</td>\n",
       "      <td>-0.034121</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>high</td>\n",
       "      <td>very_high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87048</th>\n",
       "      <td>0.067937</td>\n",
       "      <td>-0.010810</td>\n",
       "      <td>-0.015722</td>\n",
       "      <td>-0.019105</td>\n",
       "      <td>-0.027498</td>\n",
       "      <td>-0.002027</td>\n",
       "      <td>0.095356</td>\n",
       "      <td>0.143631</td>\n",
       "      <td>-0.026087</td>\n",
       "      <td>-0.087081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032202</td>\n",
       "      <td>-0.040765</td>\n",
       "      <td>0.007114</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>-0.022455</td>\n",
       "      <td>391</td>\n",
       "      <td>1</td>\n",
       "      <td>high</td>\n",
       "      <td>very_high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87049</th>\n",
       "      <td>0.055320</td>\n",
       "      <td>-0.006079</td>\n",
       "      <td>-0.009600</td>\n",
       "      <td>-0.000292</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>-0.009261</td>\n",
       "      <td>-0.034570</td>\n",
       "      <td>-0.008223</td>\n",
       "      <td>-0.030499</td>\n",
       "      <td>-0.065781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000565</td>\n",
       "      <td>-0.005678</td>\n",
       "      <td>-0.039495</td>\n",
       "      <td>-0.002250</td>\n",
       "      <td>0.040420</td>\n",
       "      <td>0.016029</td>\n",
       "      <td>315</td>\n",
       "      <td>1</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87050</th>\n",
       "      <td>0.015223</td>\n",
       "      <td>-0.000546</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>-0.002809</td>\n",
       "      <td>-0.010256</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>-0.009465</td>\n",
       "      <td>-0.020684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025513</td>\n",
       "      <td>-0.027214</td>\n",
       "      <td>-0.012491</td>\n",
       "      <td>-0.007042</td>\n",
       "      <td>0.009193</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87051 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic_1   topic_2   topic_3   topic_4   topic_5   topic_6   topic_7  \\\n",
       "0      0.182937 -0.037876 -0.021556 -0.031425 -0.015129 -0.019524  0.037902   \n",
       "1      0.081317 -0.005818 -0.003718  0.037146  0.031008  0.012801 -0.033361   \n",
       "2      0.072941 -0.001708  0.006482  0.032298  0.029774 -0.022186 -0.071312   \n",
       "3      0.094846 -0.000364  0.005569  0.033342  0.028184  0.005248 -0.047884   \n",
       "4      0.114763 -0.012122 -0.005755 -0.001325 -0.017749 -0.008861  0.011973   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "87046  0.056939 -0.014500 -0.009422 -0.012278 -0.012643 -0.000716 -0.001827   \n",
       "87047  0.015847 -0.000076 -0.005971 -0.006066  0.000145 -0.003124 -0.012159   \n",
       "87048  0.067937 -0.010810 -0.015722 -0.019105 -0.027498 -0.002027  0.095356   \n",
       "87049  0.055320 -0.006079 -0.009600 -0.000292  0.005679 -0.009261 -0.034570   \n",
       "87050  0.015223 -0.000546 -0.002052  0.003797  0.002746 -0.002809 -0.010256   \n",
       "\n",
       "        topic_8   topic_9  topic_10  ...  topic_65  topic_66  topic_67  \\\n",
       "0      0.042148 -0.059362  0.178819  ... -0.024154 -0.010394  0.040280   \n",
       "1      0.013321 -0.000854  0.008429  ...  0.017245 -0.103015  0.037871   \n",
       "2      0.007261 -0.030092  0.052803  ...  0.064571 -0.039876  0.137298   \n",
       "3      0.041983 -0.050951  0.016491  ...  0.060896 -0.045629  0.019040   \n",
       "4      0.004776  0.010265  0.024023  ... -0.023851  0.007332  0.012216   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "87046 -0.006371 -0.001868 -0.014049  ... -0.061815 -0.002758 -0.020445   \n",
       "87047  0.003439 -0.002020 -0.035379  ...  0.034951 -0.005332  0.003603   \n",
       "87048  0.143631 -0.026087 -0.087081  ... -0.032202 -0.040765  0.007114   \n",
       "87049 -0.008223 -0.030499 -0.065781  ... -0.000565 -0.005678 -0.039495   \n",
       "87050  0.003311 -0.009465 -0.020684  ...  0.025513 -0.027214 -0.012491   \n",
       "\n",
       "       topic_68  topic_69  topic_70  len_review  is_fake_review  \\\n",
       "0      0.048726 -0.035746 -0.012122         359               0   \n",
       "1     -0.067699  0.031082  0.021013         256               0   \n",
       "2      0.015482 -0.097854  0.007026         323               0   \n",
       "3      0.082028  0.021663  0.085922         247               0   \n",
       "4     -0.115549  0.082958 -0.088175         280               0   \n",
       "...         ...       ...       ...         ...             ...   \n",
       "87046  0.017416  0.016560 -0.022601         195               1   \n",
       "87047  0.042685  0.046822 -0.034121          83               1   \n",
       "87048  0.000368  0.003463 -0.022455         391               1   \n",
       "87049 -0.002250  0.040420  0.016029         315               1   \n",
       "87050 -0.007042  0.009193  0.009766          87               1   \n",
       "\n",
       "       upper_word_count  exclam_count  \n",
       "0                   low          high  \n",
       "1                  high          high  \n",
       "2                   low          high  \n",
       "3                  high           low  \n",
       "4                   low           low  \n",
       "...                 ...           ...  \n",
       "87046              high     very_high  \n",
       "87047              high     very_high  \n",
       "87048              high     very_high  \n",
       "87049              high          high  \n",
       "87050              high          high  \n",
       "\n",
       "[87051 rows x 74 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_64</th>\n",
       "      <th>topic_65</th>\n",
       "      <th>topic_66</th>\n",
       "      <th>topic_67</th>\n",
       "      <th>topic_68</th>\n",
       "      <th>topic_69</th>\n",
       "      <th>topic_70</th>\n",
       "      <th>len_review</th>\n",
       "      <th>upper_word_count</th>\n",
       "      <th>exclam_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.182937</td>\n",
       "      <td>-0.037876</td>\n",
       "      <td>-0.021556</td>\n",
       "      <td>-0.031425</td>\n",
       "      <td>-0.015129</td>\n",
       "      <td>-0.019524</td>\n",
       "      <td>0.037902</td>\n",
       "      <td>0.042148</td>\n",
       "      <td>-0.059362</td>\n",
       "      <td>0.178819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005719</td>\n",
       "      <td>-0.024154</td>\n",
       "      <td>-0.010394</td>\n",
       "      <td>0.040280</td>\n",
       "      <td>0.048726</td>\n",
       "      <td>-0.035746</td>\n",
       "      <td>-0.012122</td>\n",
       "      <td>359</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081317</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.003718</td>\n",
       "      <td>0.037146</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.012801</td>\n",
       "      <td>-0.033361</td>\n",
       "      <td>0.013321</td>\n",
       "      <td>-0.000854</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.017245</td>\n",
       "      <td>-0.103015</td>\n",
       "      <td>0.037871</td>\n",
       "      <td>-0.067699</td>\n",
       "      <td>0.031082</td>\n",
       "      <td>0.021013</td>\n",
       "      <td>256</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072941</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.032298</td>\n",
       "      <td>0.029774</td>\n",
       "      <td>-0.022186</td>\n",
       "      <td>-0.071312</td>\n",
       "      <td>0.007261</td>\n",
       "      <td>-0.030092</td>\n",
       "      <td>0.052803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144618</td>\n",
       "      <td>0.064571</td>\n",
       "      <td>-0.039876</td>\n",
       "      <td>0.137298</td>\n",
       "      <td>0.015482</td>\n",
       "      <td>-0.097854</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>323</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.094846</td>\n",
       "      <td>-0.000364</td>\n",
       "      <td>0.005569</td>\n",
       "      <td>0.033342</td>\n",
       "      <td>0.028184</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>-0.047884</td>\n",
       "      <td>0.041983</td>\n",
       "      <td>-0.050951</td>\n",
       "      <td>0.016491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>0.060896</td>\n",
       "      <td>-0.045629</td>\n",
       "      <td>0.019040</td>\n",
       "      <td>0.082028</td>\n",
       "      <td>0.021663</td>\n",
       "      <td>0.085922</td>\n",
       "      <td>247</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.114763</td>\n",
       "      <td>-0.012122</td>\n",
       "      <td>-0.005755</td>\n",
       "      <td>-0.001325</td>\n",
       "      <td>-0.017749</td>\n",
       "      <td>-0.008861</td>\n",
       "      <td>0.011973</td>\n",
       "      <td>0.004776</td>\n",
       "      <td>0.010265</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054399</td>\n",
       "      <td>-0.023851</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.012216</td>\n",
       "      <td>-0.115549</td>\n",
       "      <td>0.082958</td>\n",
       "      <td>-0.088175</td>\n",
       "      <td>280</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_1   topic_2   topic_3   topic_4   topic_5   topic_6   topic_7  \\\n",
       "0  0.182937 -0.037876 -0.021556 -0.031425 -0.015129 -0.019524  0.037902   \n",
       "1  0.081317 -0.005818 -0.003718  0.037146  0.031008  0.012801 -0.033361   \n",
       "2  0.072941 -0.001708  0.006482  0.032298  0.029774 -0.022186 -0.071312   \n",
       "3  0.094846 -0.000364  0.005569  0.033342  0.028184  0.005248 -0.047884   \n",
       "4  0.114763 -0.012122 -0.005755 -0.001325 -0.017749 -0.008861  0.011973   \n",
       "\n",
       "    topic_8   topic_9  topic_10  ...  topic_64  topic_65  topic_66  topic_67  \\\n",
       "0  0.042148 -0.059362  0.178819  ...  0.005719 -0.024154 -0.010394  0.040280   \n",
       "1  0.013321 -0.000854  0.008429  ...  0.002275  0.017245 -0.103015  0.037871   \n",
       "2  0.007261 -0.030092  0.052803  ... -0.144618  0.064571 -0.039876  0.137298   \n",
       "3  0.041983 -0.050951  0.016491  ...  0.010744  0.060896 -0.045629  0.019040   \n",
       "4  0.004776  0.010265  0.024023  ...  0.054399 -0.023851  0.007332  0.012216   \n",
       "\n",
       "   topic_68  topic_69  topic_70  len_review  upper_word_count  exclam_count  \n",
       "0  0.048726 -0.035746 -0.012122         359               low          high  \n",
       "1 -0.067699  0.031082  0.021013         256              high          high  \n",
       "2  0.015482 -0.097854  0.007026         323               low          high  \n",
       "3  0.082028  0.021663  0.085922         247              high           low  \n",
       "4 -0.115549  0.082958 -0.088175         280               low           low  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split X\n",
    "X_cl = data_cl.drop(columns=\"is_fake_review\")\n",
    "X_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        359\n",
       "1        256\n",
       "2        323\n",
       "3        247\n",
       "4        280\n",
       "        ... \n",
       "87046    195\n",
       "87047     83\n",
       "87048    391\n",
       "87049    315\n",
       "87050     87\n",
       "Name: len_review, Length: 87051, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cl.iloc[: , -3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split y \n",
    "y = data_cl[\"is_fake_review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cl,y,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    stratify = y , ## Statify splitting when you're training a classification model !\n",
    "                                                    random_state = 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for numeric features\n",
    "numeric_features = [-3] # Positions of numeric columns in X_train/X_test\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for categorical features\n",
    "categorical_features = [-1,-2] # Positions of categorical columns in X_train/X_test\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('encoder', OneHotEncoder(drop='first')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.663947    1.          0.          1.          0.        ]\n",
      " [-0.27339474  1.          0.          1.          0.        ]\n",
      " [-0.75477311  0.          1.          0.          0.        ]\n",
      " [-0.88647097  1.          0.          0.          1.        ]\n",
      " [-0.78656225  0.          0.          0.          0.        ]]\n",
      "[[ 0.13078144  0.          1.          1.          0.        ]\n",
      " [ 3.57763224  0.          0.          0.          0.        ]\n",
      " [-0.34832628  1.          0.          0.          1.        ]\n",
      " [ 0.17846515  0.          0.          0.          1.        ]\n",
      " [ 0.87782618  0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessings on train set\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "print(X_train[0:5,:])\n",
    "\n",
    "# Preprocessings on test set\n",
    "X_test = preprocessor.transform(X_test) \n",
    "print(X_test[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-81-e88f1251da1c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[\"len_review\"] = scaler.fit_transform(X_train[[\"len_review\"]])\n",
      "<ipython-input-81-e88f1251da1c>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"len_review\"] = scaler.transform(X_test[[\"len_review\"]])\n"
     ]
    }
   ],
   "source": [
    "#scaler = StandardScaler()\n",
    "#X_train[\"len_review\"] = scaler.fit_transform(X_train[[\"len_review\"]])\n",
    "#X_test[\"len_review\"] = scaler.transform(X_test[[\"len_review\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1 ..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1, total= 1.9min\n",
      "[CV] C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1 ..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1, total= 1.6min\n",
      "[CV] C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1, total= 1.7min\n",
      "[CV] C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1, total= 1.8min\n",
      "[CV] C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1, total= 1.7min\n",
      "[CV] C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1, total= 1.9min\n",
      "[CV] C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1, total= 1.8min\n",
      "[CV] C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1, total= 1.6min\n",
      "[CV] C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1, total= 1.4min\n",
      "[CV] C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1, total= 1.7min\n",
      "[CV] C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1 ....................\n",
      "[CV] ..... C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1, total= 1.6min\n",
      "[CV] C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1 ....................\n",
      "[CV] ..... C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1, total= 1.5min\n",
      "[CV] C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1 ....................\n",
      "[CV] ..... C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1, total= 1.5min\n",
      "[CV] C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1 ....................\n",
      "[CV] ..... C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1, total= 1.4min\n",
      "[CV] C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1 ....................\n",
      "[CV] ..... C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1, total= 1.4min\n",
      "[CV] C=10, class_weight=balanced, gamma=1 ............................\n",
      "[CV] ............. C=10, class_weight=balanced, gamma=1, total= 3.8min\n",
      "[CV] C=10, class_weight=balanced, gamma=1 ............................\n",
      "[CV] ............. C=10, class_weight=balanced, gamma=1, total= 4.3min\n",
      "[CV] C=10, class_weight=balanced, gamma=1 ............................\n",
      "[CV] ............. C=10, class_weight=balanced, gamma=1, total= 3.3min\n",
      "[CV] C=10, class_weight=balanced, gamma=1 ............................\n",
      "[CV] ............. C=10, class_weight=balanced, gamma=1, total= 3.7min\n",
      "[CV] C=10, class_weight=balanced, gamma=1 ............................\n",
      "[CV] ............. C=10, class_weight=balanced, gamma=1, total= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 43.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True),\n",
       "             estimator=SVC(),\n",
       "             param_grid={'C': [10],\n",
       "                         'class_weight': [{0: 0.33, 1: 0.67},\n",
       "                                          {0: 0.25, 1: 0.75}, {0: 0.2, 1: 0.8},\n",
       "                                          'balanced'],\n",
       "                         'gamma': [1]},\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits = 5, shuffle=True, random_state=0) \n",
    "\n",
    "parameters= {'C': [10], \\\n",
    "            'gamma': [1] ,\n",
    "             \"class_weight\": [{1:0.67, 0:0.33}, {1:0.75, 0:0.25}, {1:0.8, 0:0.2}, \"balanced\"] \\\n",
    "           }\n",
    "\n",
    "model = SVC()\n",
    "model_svc =GridSearchCV(model, parameters, cv=kfold, verbose=2, scoring=\"f1\")\n",
    "model_svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = model_svc.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, class_weight={0: 0.33, 1: 0.67}, gamma=1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = svc_clf.predict(X_test)\n",
    "train_pred = svc_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for model on test set\n",
      "\n",
      "Accuracy Score : 0.8770317615300672\n",
      "Precision Score : 0.5947910357359176\n",
      "Recall Score : 0.7097940007228045\n",
      "F1 Score : 0.6472235953204811\n",
      "\n",
      "\n",
      "Scores for model on train set\n",
      "\n",
      "Accuracy Score : 0.8775703618609995\n",
      "Precision Score : 0.5962121212121212\n",
      "Recall Score : 0.7111874209289716\n",
      "F1 Score : 0.6486441935218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "    \n",
    "print(\"Scores for model on test set\")\n",
    "print(\"\")\n",
    "print('Accuracy Score : {}'.format(str(accuracy_score(y_test,test_pred))))\n",
    "print('Precision Score : {}'.format(str(precision_score(y_test,test_pred))))\n",
    "print('Recall Score : {}' .format(str(recall_score(y_test,test_pred ))))\n",
    "print('F1 Score : {}'.format(str(f1_score(y_test,test_pred))))\n",
    "    \n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Scores for model on train set\")\n",
    "print(\"\")\n",
    "print('Accuracy Score : {}'.format(str(accuracy_score(y_train,train_pred))))\n",
    "print('Precision Score : {}'.format(str(precision_score(y_train,train_pred))))\n",
    "print('Recall Score : {}' .format(str(recall_score(y_train,train_pred))))\n",
    "print('F1 Score : {}'.format(str(f1_score(y_train,train_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf2 = SVC(C=10, class_weight={0: 0.33, 1: 0.67}, gamma=1,  probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, class_weight={0: 0.33, 1: 0.67}, gamma=1, probability=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_pred = X_cl.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_64</th>\n",
       "      <th>topic_65</th>\n",
       "      <th>topic_66</th>\n",
       "      <th>topic_67</th>\n",
       "      <th>topic_68</th>\n",
       "      <th>topic_69</th>\n",
       "      <th>topic_70</th>\n",
       "      <th>len_review</th>\n",
       "      <th>upper_word_count</th>\n",
       "      <th>exclam_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.182937</td>\n",
       "      <td>-0.037876</td>\n",
       "      <td>-0.021556</td>\n",
       "      <td>-0.031425</td>\n",
       "      <td>-0.015129</td>\n",
       "      <td>-0.019524</td>\n",
       "      <td>0.037902</td>\n",
       "      <td>0.042148</td>\n",
       "      <td>-0.059362</td>\n",
       "      <td>0.178819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005719</td>\n",
       "      <td>-0.024154</td>\n",
       "      <td>-0.010394</td>\n",
       "      <td>0.040280</td>\n",
       "      <td>0.048726</td>\n",
       "      <td>-0.035746</td>\n",
       "      <td>-0.012122</td>\n",
       "      <td>359</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081317</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-0.003718</td>\n",
       "      <td>0.037146</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.012801</td>\n",
       "      <td>-0.033361</td>\n",
       "      <td>0.013321</td>\n",
       "      <td>-0.000854</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.017245</td>\n",
       "      <td>-0.103015</td>\n",
       "      <td>0.037871</td>\n",
       "      <td>-0.067699</td>\n",
       "      <td>0.031082</td>\n",
       "      <td>0.021013</td>\n",
       "      <td>256</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072941</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.032298</td>\n",
       "      <td>0.029774</td>\n",
       "      <td>-0.022186</td>\n",
       "      <td>-0.071312</td>\n",
       "      <td>0.007261</td>\n",
       "      <td>-0.030092</td>\n",
       "      <td>0.052803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144618</td>\n",
       "      <td>0.064571</td>\n",
       "      <td>-0.039876</td>\n",
       "      <td>0.137298</td>\n",
       "      <td>0.015482</td>\n",
       "      <td>-0.097854</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>323</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.094846</td>\n",
       "      <td>-0.000364</td>\n",
       "      <td>0.005569</td>\n",
       "      <td>0.033342</td>\n",
       "      <td>0.028184</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>-0.047884</td>\n",
       "      <td>0.041983</td>\n",
       "      <td>-0.050951</td>\n",
       "      <td>0.016491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>0.060896</td>\n",
       "      <td>-0.045629</td>\n",
       "      <td>0.019040</td>\n",
       "      <td>0.082028</td>\n",
       "      <td>0.021663</td>\n",
       "      <td>0.085922</td>\n",
       "      <td>247</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.114763</td>\n",
       "      <td>-0.012122</td>\n",
       "      <td>-0.005755</td>\n",
       "      <td>-0.001325</td>\n",
       "      <td>-0.017749</td>\n",
       "      <td>-0.008861</td>\n",
       "      <td>0.011973</td>\n",
       "      <td>0.004776</td>\n",
       "      <td>0.010265</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054399</td>\n",
       "      <td>-0.023851</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.012216</td>\n",
       "      <td>-0.115549</td>\n",
       "      <td>0.082958</td>\n",
       "      <td>-0.088175</td>\n",
       "      <td>280</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87046</th>\n",
       "      <td>0.056939</td>\n",
       "      <td>-0.014500</td>\n",
       "      <td>-0.009422</td>\n",
       "      <td>-0.012278</td>\n",
       "      <td>-0.012643</td>\n",
       "      <td>-0.000716</td>\n",
       "      <td>-0.001827</td>\n",
       "      <td>-0.006371</td>\n",
       "      <td>-0.001868</td>\n",
       "      <td>-0.014049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043188</td>\n",
       "      <td>-0.061815</td>\n",
       "      <td>-0.002758</td>\n",
       "      <td>-0.020445</td>\n",
       "      <td>0.017416</td>\n",
       "      <td>0.016560</td>\n",
       "      <td>-0.022601</td>\n",
       "      <td>195</td>\n",
       "      <td>high</td>\n",
       "      <td>very_high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87047</th>\n",
       "      <td>0.015847</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.005971</td>\n",
       "      <td>-0.006066</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>-0.003124</td>\n",
       "      <td>-0.012159</td>\n",
       "      <td>0.003439</td>\n",
       "      <td>-0.002020</td>\n",
       "      <td>-0.035379</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034558</td>\n",
       "      <td>0.034951</td>\n",
       "      <td>-0.005332</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.042685</td>\n",
       "      <td>0.046822</td>\n",
       "      <td>-0.034121</td>\n",
       "      <td>83</td>\n",
       "      <td>high</td>\n",
       "      <td>very_high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87048</th>\n",
       "      <td>0.067937</td>\n",
       "      <td>-0.010810</td>\n",
       "      <td>-0.015722</td>\n",
       "      <td>-0.019105</td>\n",
       "      <td>-0.027498</td>\n",
       "      <td>-0.002027</td>\n",
       "      <td>0.095356</td>\n",
       "      <td>0.143631</td>\n",
       "      <td>-0.026087</td>\n",
       "      <td>-0.087081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019538</td>\n",
       "      <td>-0.032202</td>\n",
       "      <td>-0.040765</td>\n",
       "      <td>0.007114</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>-0.022455</td>\n",
       "      <td>391</td>\n",
       "      <td>high</td>\n",
       "      <td>very_high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87049</th>\n",
       "      <td>0.055320</td>\n",
       "      <td>-0.006079</td>\n",
       "      <td>-0.009600</td>\n",
       "      <td>-0.000292</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>-0.009261</td>\n",
       "      <td>-0.034570</td>\n",
       "      <td>-0.008223</td>\n",
       "      <td>-0.030499</td>\n",
       "      <td>-0.065781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025253</td>\n",
       "      <td>-0.000565</td>\n",
       "      <td>-0.005678</td>\n",
       "      <td>-0.039495</td>\n",
       "      <td>-0.002250</td>\n",
       "      <td>0.040420</td>\n",
       "      <td>0.016029</td>\n",
       "      <td>315</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87050</th>\n",
       "      <td>0.015223</td>\n",
       "      <td>-0.000546</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>-0.002809</td>\n",
       "      <td>-0.010256</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>-0.009465</td>\n",
       "      <td>-0.020684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014371</td>\n",
       "      <td>0.025513</td>\n",
       "      <td>-0.027214</td>\n",
       "      <td>-0.012491</td>\n",
       "      <td>-0.007042</td>\n",
       "      <td>0.009193</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>87</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87051 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic_1   topic_2   topic_3   topic_4   topic_5   topic_6   topic_7  \\\n",
       "0      0.182937 -0.037876 -0.021556 -0.031425 -0.015129 -0.019524  0.037902   \n",
       "1      0.081317 -0.005818 -0.003718  0.037146  0.031008  0.012801 -0.033361   \n",
       "2      0.072941 -0.001708  0.006482  0.032298  0.029774 -0.022186 -0.071312   \n",
       "3      0.094846 -0.000364  0.005569  0.033342  0.028184  0.005248 -0.047884   \n",
       "4      0.114763 -0.012122 -0.005755 -0.001325 -0.017749 -0.008861  0.011973   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "87046  0.056939 -0.014500 -0.009422 -0.012278 -0.012643 -0.000716 -0.001827   \n",
       "87047  0.015847 -0.000076 -0.005971 -0.006066  0.000145 -0.003124 -0.012159   \n",
       "87048  0.067937 -0.010810 -0.015722 -0.019105 -0.027498 -0.002027  0.095356   \n",
       "87049  0.055320 -0.006079 -0.009600 -0.000292  0.005679 -0.009261 -0.034570   \n",
       "87050  0.015223 -0.000546 -0.002052  0.003797  0.002746 -0.002809 -0.010256   \n",
       "\n",
       "        topic_8   topic_9  topic_10  ...  topic_64  topic_65  topic_66  \\\n",
       "0      0.042148 -0.059362  0.178819  ...  0.005719 -0.024154 -0.010394   \n",
       "1      0.013321 -0.000854  0.008429  ...  0.002275  0.017245 -0.103015   \n",
       "2      0.007261 -0.030092  0.052803  ... -0.144618  0.064571 -0.039876   \n",
       "3      0.041983 -0.050951  0.016491  ...  0.010744  0.060896 -0.045629   \n",
       "4      0.004776  0.010265  0.024023  ...  0.054399 -0.023851  0.007332   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "87046 -0.006371 -0.001868 -0.014049  ... -0.043188 -0.061815 -0.002758   \n",
       "87047  0.003439 -0.002020 -0.035379  ... -0.034558  0.034951 -0.005332   \n",
       "87048  0.143631 -0.026087 -0.087081  ...  0.019538 -0.032202 -0.040765   \n",
       "87049 -0.008223 -0.030499 -0.065781  ... -0.025253 -0.000565 -0.005678   \n",
       "87050  0.003311 -0.009465 -0.020684  ... -0.014371  0.025513 -0.027214   \n",
       "\n",
       "       topic_67  topic_68  topic_69  topic_70  len_review  upper_word_count  \\\n",
       "0      0.040280  0.048726 -0.035746 -0.012122         359               low   \n",
       "1      0.037871 -0.067699  0.031082  0.021013         256              high   \n",
       "2      0.137298  0.015482 -0.097854  0.007026         323               low   \n",
       "3      0.019040  0.082028  0.021663  0.085922         247              high   \n",
       "4      0.012216 -0.115549  0.082958 -0.088175         280               low   \n",
       "...         ...       ...       ...       ...         ...               ...   \n",
       "87046 -0.020445  0.017416  0.016560 -0.022601         195              high   \n",
       "87047  0.003603  0.042685  0.046822 -0.034121          83              high   \n",
       "87048  0.007114  0.000368  0.003463 -0.022455         391              high   \n",
       "87049 -0.039495 -0.002250  0.040420  0.016029         315              high   \n",
       "87050 -0.012491 -0.007042  0.009193  0.009766          87              high   \n",
       "\n",
       "       exclam_count  \n",
       "0              high  \n",
       "1              high  \n",
       "2              high  \n",
       "3               low  \n",
       "4               low  \n",
       "...             ...  \n",
       "87046     very_high  \n",
       "87047     very_high  \n",
       "87048     very_high  \n",
       "87049          high  \n",
       "87050          high  \n",
       "\n",
       "[87051 rows x 73 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_pred = preprocessor.transform(X_final_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svc_clf2.predict_proba(X_final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "87046    1\n",
       "87047    1\n",
       "87048    1\n",
       "87049    1\n",
       "87050    1\n",
       "Name: is_fake_review, Length: 87051, dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cl.is_fake_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svm_nlp = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svm_nlp[3] = data_cl.is_fake_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61455</th>\n",
       "      <td>0.958150</td>\n",
       "      <td>0.041850</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15044</th>\n",
       "      <td>0.958839</td>\n",
       "      <td>0.041161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4216</th>\n",
       "      <td>0.957377</td>\n",
       "      <td>0.042623</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71363</th>\n",
       "      <td>0.760686</td>\n",
       "      <td>0.239314</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.939691</td>\n",
       "      <td>0.060309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45542</th>\n",
       "      <td>0.975652</td>\n",
       "      <td>0.024348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79822</th>\n",
       "      <td>0.341989</td>\n",
       "      <td>0.658011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51552</th>\n",
       "      <td>0.763849</td>\n",
       "      <td>0.236151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67639</th>\n",
       "      <td>0.940260</td>\n",
       "      <td>0.059740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61252</th>\n",
       "      <td>0.938742</td>\n",
       "      <td>0.061258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46046</th>\n",
       "      <td>0.938532</td>\n",
       "      <td>0.061468</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8718</th>\n",
       "      <td>0.939739</td>\n",
       "      <td>0.060261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28052</th>\n",
       "      <td>0.939289</td>\n",
       "      <td>0.060711</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68240</th>\n",
       "      <td>0.939372</td>\n",
       "      <td>0.060628</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23026</th>\n",
       "      <td>0.966822</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15416</th>\n",
       "      <td>0.941523</td>\n",
       "      <td>0.058477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77712</th>\n",
       "      <td>0.941371</td>\n",
       "      <td>0.058629</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44307</th>\n",
       "      <td>0.980156</td>\n",
       "      <td>0.019844</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8105</th>\n",
       "      <td>0.959641</td>\n",
       "      <td>0.040359</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35200</th>\n",
       "      <td>0.957887</td>\n",
       "      <td>0.042113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1  3\n",
       "61455  0.958150  0.041850  0\n",
       "15044  0.958839  0.041161  0\n",
       "4216   0.957377  0.042623  0\n",
       "71363  0.760686  0.239314  0\n",
       "224    0.939691  0.060309  0\n",
       "45542  0.975652  0.024348  0\n",
       "79822  0.341989  0.658011  1\n",
       "51552  0.763849  0.236151  0\n",
       "67639  0.940260  0.059740  0\n",
       "61252  0.938742  0.061258  0\n",
       "46046  0.938532  0.061468  0\n",
       "8718   0.939739  0.060261  0\n",
       "28052  0.939289  0.060711  0\n",
       "68240  0.939372  0.060628  0\n",
       "23026  0.966822  0.033178  0\n",
       "15416  0.941523  0.058477  0\n",
       "77712  0.941371  0.058629  1\n",
       "44307  0.980156  0.019844  0\n",
       "8105   0.959641  0.040359  0\n",
       "35200  0.957887  0.042113  0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_svm_nlp.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/fake_reviews_raw.csv\n",
      "datasets/full_dataset.csv\n",
      "datasets/full_dataset_reworked.csv\n",
      "datasets/predictions_svm_nlp.csv\n",
      "datasets/real_reviews_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# set path and bucket name\n",
    "PATH = \"datasets/predictions_svm_nlp.csv\"\n",
    "bucket = s3.Bucket(name = \"jedha-fake-reviews-project\")\n",
    "# export dataset as csv\n",
    "data = predictions_svm_nlp.to_csv()\n",
    "\n",
    "#upload to bucket\n",
    "put_object = bucket.put_object(ACL='private', Key= PATH, Body=data)\n",
    "#check \n",
    "for obj in bucket.objects.all():\n",
    "    print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"/Users/personal/Dropbox/dataset_tableau.csv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
