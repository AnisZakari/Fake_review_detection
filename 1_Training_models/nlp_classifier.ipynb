{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to s3 instance\n",
    "import boto3\n",
    "YOUR_ACCESS_KEY = \"\"\n",
    "YOUR_SECRET_KEY = \"\"\n",
    "\n",
    "session = boto3.Session(aws_access_key_id= YOUR_ACCESS_KEY, \n",
    "                        aws_secret_access_key= YOUR_SECRET_KEY)\n",
    "\n",
    "s3 = session.resource(\"s3\")\n",
    "client = session.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real\n",
    "obj = s3.Object('jedha-fake-reviews-project', \"datasets/full_dataset.csv\")\n",
    "full_dataset = pd.read_csv(io.BytesIO(obj.get()['Body'].read()), low_memory = False, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = full_dataset[full_dataset[\"language\"] == \"fr\"].sample(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample[[\"text_review\", \"language\", \"is_real_review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sample.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"len_review\"] = data[\"text_review\"].apply(lambda x : len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text_review_clean\"] = data[\"text_review\"].str.strip()\n",
    "\n",
    "#lower\n",
    "data[\"text_review_clean\"] = data[\"text_review_clean\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text_review_clean\"] = data[\"text_review_clean\"].str.replace(r\"<[a-z/]+>\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# def function\n",
    "def remove_punctuation(text): \n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# apply to column\n",
    "data[\"text_review_clean\"] = data[\"text_review_clean\"].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing, lemmatizing and deleteing stopwords from doc with Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first let's find the count of all words and return them in the form of dict items\n",
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(' '.join(data[\"text_review_clean\"]).split()).items() #\n",
    "print(len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with all words and their count\n",
    "word_count = pd.DataFrame({'word': [item[0] for item in list(word_count)], \n",
    "             'count' : [item[1] for item in list (word_count)]})\n",
    "\n",
    "# format\n",
    "word_count = word_count.sort_values('count', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_count.shape)\n",
    "word_count.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take all words that occur more than 500 times\n",
    "commonwords = word_count.loc[word_count[\"count\"]>=2000, :]\n",
    "commonwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create nlp instance\n",
    "nlp =  fr_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lemmatize common words \n",
    "commonwords[\"word\"] = commonwords[\"word\"].apply(lambda x: nlp(x))\n",
    "commonwords[\"word\"] = commonwords[\"word\"].apply(lambda x: [token.lemma_ for token in x])\n",
    "commonwords.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join\n",
    "commonwords[\"word\"] = commonwords[\"word\"].str.join(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list\n",
    "common_words = commonwords.word\n",
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append to stopwords \n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "print(len(STOP_WORDS))\n",
    "STOP_WORDS_MAX = STOP_WORDS.union(common_words)\n",
    "\n",
    "# also add the lemmatizer for pronouns as we won't need them\n",
    "STOP_WORDS_MAX.add(\"-PRON-\")\n",
    "print(len(STOP_WORDS_MAX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  apply nlp to transform into doc\n",
    "data[\"clean_tokens\"] = data[\"text_review_clean\"].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize each token and remove stop words --> could be done in two steps but we do it in one\n",
    "data['clean_tokens_lemmatized'] = data['clean_tokens'].apply(lambda doc: [token.lemma_ for token in doc if token.lemma_ not in STOP_WORDS_MAX])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### join all of them into new df column\n",
    "# method 1\n",
    "data[\"clean_review\"] = data[\"clean_tokens_lemmatized\"].str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a TFIDF Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply vectorizer to the review column\n",
    "vectorizer = TfidfVectorizer(smooth_idf=True)\n",
    "X = vectorizer.fit_transform(data['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform this sparse matrix into a numpy array \n",
    "X_dense = X.toarray()\n",
    "print(X_dense.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check out the vocabulary of this doc\n",
    "print(len(vectorizer.vocabulary_))\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's put the matrix into a DF with the feature name (ie word) as column title and the document number as ID\n",
    "# this is easily doable because the get_feature_names method of vectorizer returns the feature names \n",
    "# with the same index as their values in the X_dense matrix\n",
    "X_df = pd.DataFrame(X_dense, \n",
    "             columns=[x for x in vectorizer.get_feature_names()], \n",
    "             index=[\"review_{}\".format(i) for i in range (1,30001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from sklearn\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set it to 12 different topics \n",
    "svd = TruncatedSVD(n_components= 80)\n",
    "\n",
    "# fit to our matrix --> last two columns are those with the previous cluster_values\n",
    "lsa = svd.fit_transform(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_encoded_df = pd.DataFrame(lsa, columns = [\"topic_{}\".format(i) \\\n",
    "                                                for i in range(1,(lsa.shape[1]+1))]\\\n",
    "                               )\n",
    "topic_encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data For Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cl = topic_encoded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cl[\"is_real_review\"] = list(data[\"is_real_review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cl[\"len_review\"] = list(data.len_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cl.groupby(\"is_real_review\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cl[\"is_fake_review\"] = data_cl[\"is_real_review\"].apply(lambda x: '1' if x == 0 else '0')\n",
    "data_cl[\"is_fake_review\"] = data_cl[\"is_fake_review\"].astype(int)\n",
    "data_cl = data_cl.drop(columns=\"is_real_review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_cl = data_cl.drop(columns=\"is_fake_review\")\n",
    "X_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_cl[\"is_fake_review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cl,y,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    stratify = y , ## Statify splitting when you're training a classification model !\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train[\"len_review\"] = scaler.fit_transform(X_train[[\"len_review\"]])\n",
    "X_test[\"len_review\"] = scaler.transform(X_test[[\"len_review\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining a function that prints out the scores of a given classifier\n",
    "def print_scores(model_name, X_train = X_train , X_test = X_test, y_test = y_test , y_train = y_train): \n",
    "    \n",
    "    from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "    \n",
    "    print(\"Scores for model on test set\")\n",
    "    print(\"\")\n",
    "    print('Accuracy Score : {}'.format(str(accuracy_score(y_test,model_name.predict(X_test)))))\n",
    "    print('Precision Score : {}'.format(str(precision_score(y_test,model_name.predict(X_test)))))\n",
    "    print('Recall Score : {}' .format(str(recall_score(y_test,model_name.predict(X_test)))))\n",
    "    print('F1 Score : {}'.format(str(f1_score(y_test,model_name.predict(X_test)))))\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"Scores for model on train set\")\n",
    "    print(\"\")\n",
    "    print('Accuracy Score : {}'.format(str(accuracy_score(y_train,model_name.predict(X_train)))))\n",
    "    print('Precision Score : {}'.format(str(precision_score(y_train,model_name.predict(X_train)))))\n",
    "    print('Recall Score : {}' .format(str(recall_score(y_train,model_name.predict(X_train)))))\n",
    "    print('F1 Score : {}'.format(str(f1_score(y_train,model_name.predict(X_train)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function that prints confusion matrix \n",
    "\n",
    "def show_confusion_matrix(model_name,X_train = X_train , X_test = X_test, y_test = y_test , y_train = y_train ): # def_function to show confusion_matrix\n",
    "\n",
    "    import matplotlib.pyplot as plt \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10, 4))  \n",
    "\n",
    "    ax1.set_title('Confusion Matrix of the test set')\n",
    "    ax1.set_xlabel(\"Predicted Values\")\n",
    "    ax1.set_ylabel(\"Actual Values\")\n",
    "    \n",
    "    ax2.set_title('Confusion Matrix of the train set')\n",
    "    ax2.set_xlabel(\"Predicted Values\")\n",
    "    ax2.set_ylabel(\"Actual Values\")\n",
    "    \n",
    "    cfm_test = confusion_matrix(y_test,model_name.predict(X_test))\n",
    "    cfm_train = confusion_matrix(y_train,model_name.predict(X_train))\n",
    "    sns.heatmap(cfm_test, annot=True, fmt=\"g\", cmap=\"seismic\", ax=ax1, )\n",
    "    sns.heatmap(cfm_train, annot=True, fmt=\"g\", cmap=\"seismic\", ax=ax2)\n",
    "    \n",
    "    \n",
    "    plt.tight_layout(), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_clf = RandomForestClassifier()\n",
    "# rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search random forest CV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle=True, random_state=0) \n",
    "\n",
    "parameters= {\"criterion\": [\"gini\"], \\\n",
    "             \"class_weight\": [{1:0.67, 0:0.33}, {1:0.75, 0:0.25}, {1:0.8, 0:0.2}, \"None\", \"balanced\"], \\\n",
    "            \"max_depth\": range(5,50) , \\\n",
    "            \"min_samples_leaf\" : range(5,50), \\\n",
    "            \"min_samples_split\" : [2, 5, 10, 20, 30], \\\n",
    "            \"n_estimators\" : [10, 50, 100, 200]}\n",
    "\n",
    "model = RandomForestClassifier(\"\")\n",
    "model_rs =RandomizedSearchCV(model, parameters, cv=kfold, verbose=2, n_iter=10, scoring=\"f1\")\n",
    "model_rs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = model_rs.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(rf_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(rf_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold = StratifiedKFold(n_splits = 10, shuffle=True, random_state=0) \n",
    "\n",
    "# parameters= {'C': [1,10,100,1000], \\\n",
    "             \"class_weight\": [{1:0.67, 0:0.33}, {1:0.75, 0:0.25}, {1:0.8, 0:0.2}, \"None\", \"balanced\"], \\\n",
    "            'gamma': [1,0.1,0.001,0.0001] , \\\n",
    "            'kernel':['linear','rbf']}\n",
    "\n",
    "# model = SVC(\"\")\n",
    "# model_svc =RandomizedSearchCV(model, parameters, cv=kfold, verbose=2, n_iter=10, scoring=\"f1\")\n",
    "# model_svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_clf = model_rs.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(svc_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(svc_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
