{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Clean DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "import fr_core_news_sm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = fr_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/admin/Jedha\n"
     ]
    }
   ],
   "source": [
    "cd '/Users/admin/Jedha/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3_credentials import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to s3 instance\n",
    "import boto3\n",
    "#YOUR_ACCESS_KEY = \n",
    "#YOUR_SECRET_KEY = \n",
    "\n",
    "session = boto3.Session(aws_access_key_id= YOUR_ACCESS_KEY, \n",
    "                        aws_secret_access_key= YOUR_SECRET_KEY)\n",
    "\n",
    "s3 = session.resource(\"s3\")\n",
    "client = session.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full DS\n",
    "obj = s3.Object('jedha-fake-reviews-project', \"datasets/full_dataset.csv\")\n",
    "dataset = pd.read_csv(io.BytesIO(obj.get()['Body'].read()), low_memory = False, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____________________________________________________________________\n",
    "######### Cleaning the dataset and adding new columns #########\n",
    "#_____________________________________________________________________\n",
    "\n",
    "#we drop rows in which restaurant infos are not available (miss scraped)\n",
    "dataset = dataset.dropna(subset = ['restaurant_average_rating', 'restaurant_reviews_count', 'restaurant_expensiveness', 'restaurant_name'])\n",
    "\n",
    "#adding a column with the length of the text review\n",
    "dataset['text_length'] = dataset['text_review'].apply(lambda x : len(x))\n",
    "\n",
    "#_____________________________________________________________________\n",
    "######### Fixing existing columns values and types #########\n",
    "#_____________________________________________________________________\n",
    "\n",
    "#for the user_total_image_posted column, if user_total_image_posted is NA it means there is there's no image\n",
    "    # so we set the value to 0\n",
    "dataset.loc[dataset['user_total_image_posted'].isna(), 'user_total_image_posted'] = 0\n",
    "\n",
    "#for the date column,  there is some miss scraps that we want to fix\n",
    "    # a correct data must have a length of 10 , if it is smaller than 10 it's becasue we scrapped the number of images of the user instead\n",
    "    # we may have to scrap again those lines to fix it\n",
    "    # we keep only the rows where the date is correct \n",
    "mask_not_date = dataset['date'].apply(lambda x: len(x)) < 10\n",
    "dataset = dataset.loc[mask_not_date == False, :]\n",
    "    # if te length is greater than 10 is it is beacause we scraped the date + somme additional words ('Avis mis à jour') so we will keep only the part with the date\n",
    "mask_date_to_fix = dataset['date'].apply(lambda x: len(x)) > 10\n",
    "dataset.loc[mask_date_to_fix, 'date' ] = dataset.loc[mask_date_to_fix, 'date' ].str.split('\\n').str[0]\n",
    "    #finally we can convert the date column to a datetime format\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "\n",
    "#for the photos_for_review column, \n",
    "    # value -1 is in fact 0 (no photos found by the scraper)\n",
    "dataset.loc[dataset['photos_for_review'] == '-1.0', 'photos_for_review' ] = '0'\n",
    "    # value L is in fact 0 (no photos found by the scraper but scraped the first letter of \"L'avis du jour\" which happens when the reviews was updated by the user)\n",
    "dataset.loc[dataset['photos_for_review'] == 'L', 'photos_for_review' ] = '0'\n",
    "    # finally we can convert the photos_for_review column to an int format\n",
    "dataset['photos_for_review'] = dataset['photos_for_review'].astype('int')\n",
    "\n",
    "#for the photos_for_review column, \n",
    "    # when there's no info about the expensiveness we set it to -1\n",
    "dataset.loc[dataset['restaurant_expensiveness'] == 'N/C', 'restaurant_expensiveness']  = -1\n",
    "    # we can convert the restaurant_expensiveness column to an int format\n",
    "dataset['restaurant_expensiveness'] = dataset['restaurant_expensiveness'].astype('int')\n",
    "\n",
    "# change is real review for is fake review as it's better for sklearn \n",
    "dataset[\"is_fake_review\"] = dataset[\"is_real_review\"].apply(lambda x: '1' if x == 0 else '0')\n",
    "dataset[\"is_fake_review\"] = dataset[\"is_fake_review\"].astype(int)\n",
    "dataset = dataset.drop(columns=\"is_real_review\")\n",
    "\n",
    "# reset index \n",
    "dataset = dataset.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_reviews = dataset.loc[dataset['language'] =='fr',['text_review', 'is_fake_review']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = french_reviews.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean and Lemmatize the data\n",
    "\n",
    "#removes '/n', and any non alphanumeric character and finally if there's more than one space in a row it turns it to one space\n",
    "data[\"text_review_clean\"] = data[\"text_review\"].str.replace(r\"[\\n]*?[^A-zÀ-ÿ0-9' ]+\", ' ').str.replace(r\" +\",\" \").str.lower()\n",
    "data['text_review_clean'] = data['text_review_clean'].apply(lambda x : unidecode(str(x)))\n",
    "data[\"text_review_clean\"] = data[\"text_review_clean\"].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x) if token.lemma_ not in STOP_WORDS]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset_clean_v2.csv', index_col= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing, lemmatizing and deleteing stopwords from doc with Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_review_clean'] = data['text_review_clean'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply vectorizer to the review column\n",
    "vectorizer = TfidfVectorizer(smooth_idf=True, min_df=200)\n",
    "X = vectorizer.fit_transform(data['text_review_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2102"
      ]
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from sklearn\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set it to 12 different topics \n",
    "svd = TruncatedSVD(n_components= 500)\n",
    "# fit to our matrix --> last two columns are those with the previous cluster_values\n",
    "lsa = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lsa).to_csv('lsa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5976252366728456\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data For Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_review = data[\"text_review\"].apply(lambda x : len(str(x)))\n",
    "len_review = pd.qcut(len_review, 2, labels = ['low', 'high'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['upper_word_count'] = data['text_review'].apply(lambda x : sum(map(str.isupper, x.split())) )\n",
    "upper_word_count = pd.qcut(data['upper_word_count'].rank(method = 'first'), 3, labels = ['low', 'mid', 'high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['exclam_count'] = data['text_review'].apply(lambda x : len(''.join(ch for ch in x if ch =='!')))\n",
    "exclam_count = pd.qcut(data['exclam_count'].rank(method = 'first'), 3, labels = ['low', 'high', 'very_high'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = pd.concat([pd.DataFrame(lsa), len_review, exclam_count, upper_word_count ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split y \n",
    "y = data[\"is_fake_review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df,y, \n",
    "                                                    test_size = 0.2,\n",
    "                                                    stratify = y , ## Statify splitting when you're training a classification model !\n",
    "                                                    random_state = 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for categorical features\n",
    "\n",
    "categorical_features = [index for index, c in enumerate(df.columns) if c in ['text_review','exclam_count', 'upper_word_count' ] ]\n",
    "numerical_features = [index for index, c in enumerate(df.columns) if c not in ['text_review','exclam_count', 'upper_word_count' ] ] # Positions of categorical columns in X_train/X_test\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('encoder', OneHotEncoder(drop='first'))\n",
    "    ])# first column will be dropped to avoid creating correlations between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for numeric features\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(72477, 503)"
      ]
     },
     "metadata": {},
     "execution_count": 229
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessings on train set\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Preprocessings on test set\n",
    "X_test = preprocessor.transform(X_test) \n",
    "\n",
    "#Whole Set\n",
    "\n",
    "#X_w = preprocessor.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(72477, 505)"
      ]
     },
     "metadata": {},
     "execution_count": 231
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1 ..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1, total= 1.9min\n",
      "[CV] C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1 ..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1, total= 1.6min\n",
      "[CV] C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1, total= 1.7min\n",
      "[CV] C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1, total= 1.8min\n",
      "[CV] C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.67, 0: 0.33}, gamma=1, total= 1.7min\n",
      "[CV] C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1, total= 1.9min\n",
      "[CV] C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1, total= 1.8min\n",
      "[CV] C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1, total= 1.6min\n",
      "[CV] C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1, total= 1.4min\n",
      "[CV] C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1 ..................\n",
      "[CV] ... C=10, class_weight={1: 0.75, 0: 0.25}, gamma=1, total= 1.7min\n",
      "[CV] C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1 ....................\n",
      "[CV] ..... C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1, total= 1.6min\n",
      "[CV] C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1 ....................\n",
      "[CV] ..... C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1, total= 1.5min\n",
      "[CV] C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1 ....................\n",
      "[CV] ..... C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1, total= 1.5min\n",
      "[CV] C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1 ....................\n",
      "[CV] ..... C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1, total= 1.4min\n",
      "[CV] C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1 ....................\n",
      "[CV] ..... C=10, class_weight={1: 0.8, 0: 0.2}, gamma=1, total= 1.4min\n",
      "[CV] C=10, class_weight=balanced, gamma=1 ............................\n",
      "[CV] ............. C=10, class_weight=balanced, gamma=1, total= 3.8min\n",
      "[CV] C=10, class_weight=balanced, gamma=1 ............................\n",
      "[CV] ............. C=10, class_weight=balanced, gamma=1, total= 4.3min\n",
      "[CV] C=10, class_weight=balanced, gamma=1 ............................\n",
      "[CV] ............. C=10, class_weight=balanced, gamma=1, total= 3.3min\n",
      "[CV] C=10, class_weight=balanced, gamma=1 ............................\n",
      "[CV] ............. C=10, class_weight=balanced, gamma=1, total= 3.7min\n",
      "[CV] C=10, class_weight=balanced, gamma=1 ............................\n",
      "[CV] ............. C=10, class_weight=balanced, gamma=1, total= 3.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 43.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True),\n",
       "             estimator=SVC(),\n",
       "             param_grid={'C': [10],\n",
       "                         'class_weight': [{0: 0.33, 1: 0.67},\n",
       "                                          {0: 0.25, 1: 0.75}, {0: 0.2, 1: 0.8},\n",
       "                                          'balanced'],\n",
       "                         'gamma': [1]},\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits = 5, shuffle=True, random_state=0) \n",
    "\n",
    "parameters= {'C': [10], \\\n",
    "            'gamma': [1] ,\n",
    "             \"class_weight\": [{1:0.67, 0:0.33}, {1:0.75, 0:0.25}, {1:0.8, 0:0.2}, \"balanced\"] \\\n",
    "           }\n",
    "\n",
    "model = SVC()\n",
    "model_svc =GridSearchCV(model, parameters, cv=kfold, verbose=2, scoring=\"f1\")\n",
    "model_svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.95, class_weight={0: 1, 1: 2.1})"
      ]
     },
     "metadata": {},
     "execution_count": 232
    }
   ],
   "source": [
    "# Train model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(C=0.95, class_weight={0:1, 1:2.1}, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "f1 test : 0.695471744171854\nf1 train : 0.7045908797392455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('f1 test :',f1_score(y_test, model.predict(X_test)))\n",
    "print('f1 train :',f1_score(y_train, model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(C=1.0, break_ties=False, cache_size=200, class_weight={0:1, 1:1.8}, coef0=0.0,\n",
    "                decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
    "                max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "                tol=0.001, verbose=False)\n",
    "\n",
    "svc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('f1 test :',f1_score(y_test, svc_model.predict(X_test)))\n",
    "print('f1 train :',f1_score(y_train, svc_model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for model on test set\n",
      "\n",
      "Accuracy Score : 0.8770317615300672\n",
      "Precision Score : 0.5947910357359176\n",
      "Recall Score : 0.7097940007228045\n",
      "F1 Score : 0.6472235953204811\n",
      "\n",
      "\n",
      "Scores for model on train set\n",
      "\n",
      "Accuracy Score : 0.8775703618609995\n",
      "Precision Score : 0.5962121212121212\n",
      "Recall Score : 0.7111874209289716\n",
      "F1 Score : 0.6486441935218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "    \n",
    "print(\"Scores for model on test set\")\n",
    "print(\"\")\n",
    "print('Accuracy Score : {}'.format(str(accuracy_score(y_test,test_pred))))\n",
    "print('Precision Score : {}'.format(str(precision_score(y_test,test_pred))))\n",
    "print('Recall Score : {}' .format(str(recall_score(y_test,test_pred ))))\n",
    "print('F1 Score : {}'.format(str(f1_score(y_test,test_pred))))\n",
    "    \n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Scores for model on train set\")\n",
    "print(\"\")\n",
    "print('Accuracy Score : {}'.format(str(accuracy_score(y_train,train_pred))))\n",
    "print('Precision Score : {}'.format(str(precision_score(y_train,train_pred))))\n",
    "print('Recall Score : {}' .format(str(recall_score(y_train,train_pred))))\n",
    "print('F1 Score : {}'.format(str(f1_score(y_train,train_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf2 = SVC(C=10, class_weight={0: 0.33, 1: 0.67}, gamma=1,  probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, class_weight={0: 0.33, 1: 0.67}, gamma=1, probability=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/fake_reviews_raw.csv\n",
      "datasets/full_dataset.csv\n",
      "datasets/full_dataset_reworked.csv\n",
      "datasets/predictions_svm_nlp.csv\n",
      "datasets/real_reviews_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# set path and bucket name\n",
    "PATH = \"datasets/predictions_svm_nlp.csv\"\n",
    "bucket = s3.Bucket(name = \"jedha-fake-reviews-project\")\n",
    "# export dataset as csv\n",
    "data = predictions_svm_nlp.to_csv()\n",
    "\n",
    "#upload to bucket\n",
    "put_object = bucket.put_object(ACL='private', Key= PATH, Body=data)\n",
    "#check \n",
    "for obj in bucket.objects.all():\n",
    "    print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['text_vectorizer.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "source": [
    "\n",
    "# Save to file in the current working directory\n",
    "joblib_file = \"text_vectorizer.pkl\"\n",
    "joblib.dump(vectorizer, joblib_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['topic_extractor.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 194
    }
   ],
   "source": [
    "# Save to file in the current working directory\n",
    "joblib_file = \"topic_extractor.pkl\"\n",
    "joblib.dump(svd, joblib_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['main_model.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 195
    }
   ],
   "source": [
    "# Save to file in the current working directory\n",
    "joblib_file = \"main_model.pkl\"\n",
    "joblib.dump(model, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['preprocessor.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 234
    }
   ],
   "source": [
    "# Save to file in the current working directory\n",
    "joblib_file = \"preprocessor.pkl\"\n",
    "joblib.dump(preprocessor, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}