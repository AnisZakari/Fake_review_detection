{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import jsonfiy from Flask\n",
    "import pandas as pd\n",
    "from flask import Flask, url_for, request, jsonify, json\n",
    "from joblib import dump, load\n",
    "from werkzeug.exceptions import HTTPException\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "import fr_core_news_sm\n",
    "nlp = fr_core_news_sm.load()\n",
    "\n",
    "main_path = '/Users/admin/Jedha/Fake_reviews_detection/1_Training_models/text_only_models'\n",
    "#\n",
    "lr = load(main_path + '/main_model.pkl')\n",
    "#\n",
    "preprocessor = load(main_path + '/preprocessor.pkl')\n",
    "#\n",
    "text_vectorizer = load(main_path + '/text_vectorizer.pkl')\n",
    "#\n",
    "topic_extractor = load(main_path + '/topic_extractor.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = fr_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = review.replace('\\n', ' ')\n",
    "length = len(review)\n",
    "exclam_count = len(''.join(ch for ch in review if ch =='!'))\n",
    "uppercase_word_count = sum(map(str.isupper, review.split()))\n",
    "\n",
    "meta = pd.DataFrame([[length, uppercase_word_count, exclam_count]], columns = ['len_review', 'exclam_count', 'upper_word_count'])\n",
    "\n",
    "review_clean = re.sub(r\"[^A-zÀ-ÿ0-9' ]+\", \" \", review).lower()\n",
    "review_clean = re.sub(' +', ' ',review_clean).strip()\n",
    "lemmatized = \" \".join([token.lemma_ for token in nlp(review_clean) if token.lemma_ not in STOP_WORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = pd.DataFrame(topic)\n",
    "\n",
    "#####DOUBLE CHECK BINS\n",
    "\n",
    "#Categorizing meta values, the bins are the quantiles used to cut the original dataset (see meta_data_analysis notebook)\n",
    "#length\n",
    "#meta['length'] = pd.cut(meta['len_review'], bins=[   5.,  368., 4998.], labels = ['low', 'high'], include_lowest=True)\n",
    "#uppercase\n",
    "meta['upper_word_count'] = pd.cut(meta['upper_word_count'].rank(method = 'first'), bins=[  0.,   3,   6, 147.], labels = ['low', 'mid', 'high'], include_lowest=True)\n",
    "#exclam\n",
    "meta['exclam_count'] = pd.cut(meta['exclam_count'].rank(method = 'first'), bins=[  0.,   0.01,   2., 133.],labels=['low', 'high', 'very_high'], include_lowest=True)\n",
    "\n",
    "#We concatenate the topic and the metadata \n",
    "conc = pd.concat([topic, meta], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev(text):\n",
    "    #Cleaning\n",
    "    review = text.replace('\\n', ' ')\n",
    "    #Feature Engineering\n",
    "    length = len(review)\n",
    "    exclam_count = len(''.join(ch for ch in review if ch =='!'))\n",
    "    uppercase_word_count = sum(map(str.isupper, review.split()))\n",
    "\n",
    "    meta = pd.DataFrame([[length, uppercase_word_count, exclam_count]], columns = ['len_review', 'exclam_count', 'upper_word_count'])\n",
    "    #Cleaning, Vectorization and Topic Extraction\n",
    "\n",
    "    review_clean = re.sub(r\"[^A-zÀ-ÿ0-9' ]+\", \" \", review).lower()\n",
    "    review_clean = re.sub(' +', ' ',review_clean).strip()\n",
    "    lemmatized = \" \".join([token.lemma_ for token in nlp(review_clean) if token.lemma_ not in STOP_WORDS])\n",
    "\n",
    "    vectrorized_item = text_vectorizer.transform([lemmatized])\n",
    "    topic = topic_extractor.transform(vectrorized_item)\n",
    "\n",
    "    topic = pd.DataFrame(topic)\n",
    "\n",
    "    #####DOUBLE CHECK BINS\n",
    "\n",
    "    #Categorizing meta values, the bins are the quantiles used to cut the original dataset (see meta_data_analysis notebook)\n",
    "    #length\n",
    "    meta['len_review'] = pd.cut(meta['len_review'], bins=[   5.,  368., 4998.], labels = ['low', 'high'], include_lowest=True)\n",
    "    #uppercase\n",
    "    meta['upper_word_count'] = pd.cut(meta['upper_word_count'].rank(method = 'first'), bins=[  0.,   3,   6, 147.], labels = ['low', 'mid',    'high'], include_lowest=True)\n",
    "    #exclam\n",
    "    meta['exclam_count'] = pd.cut(meta['exclam_count'].rank(method = 'first'), bins=[  0.,   0.01,   2., 133.],labels=['low', 'high',          'very_high'], include_lowest=True)\n",
    "\n",
    "    #We concatenate the topic and the metadata \n",
    "    conc = pd.concat([topic, meta], axis = 1)\n",
    "\n",
    "    #preprocessing \n",
    "    data = preprocessor.transform(conc)\n",
    "    prev = lr.predict(data)\n",
    "\n",
    "    return prev[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/admin/Jedha\n"
     ]
    }
   ],
   "source": [
    "cd '/Users/admin/Jedha/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}