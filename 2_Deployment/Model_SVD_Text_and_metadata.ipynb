{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import jsonfiy from Flask\n",
    "import pandas as pd\n",
    "from flask import Flask, url_for, request, jsonify, json\n",
    "from joblib import dump, load\n",
    "from werkzeug.exceptions import HTTPException\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "import fr_core_news_sm\n",
    "nlp = fr_core_news_sm.load()\n",
    "\n",
    "main_path = '/Users/admin/Jedha/Fake_reviews_detection/1_Training_models/text_only_models'\n",
    "#\n",
    "lr = load(main_path + '/main_model.pkl')\n",
    "#\n",
    "preprocessor = load(main_path + '/preprocessor.pkl')\n",
    "#\n",
    "text_vectorizer = load(main_path + '/text_vectorizer.pkl')\n",
    "#\n",
    "topic_extractor = load(main_path + '/topic_extractor.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = fr_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = review.replace('\\n', ' ')\n",
    "length = len(review)\n",
    "exclam_count = len(''.join(ch for ch in review if ch =='!'))\n",
    "uppercase_word_count = sum(map(str.isupper, review.split()))\n",
    "\n",
    "meta = pd.DataFrame([[length, uppercase_word_count, exclam_count]], columns = ['len_review', 'exclam_count', 'upper_word_count'])\n",
    "\n",
    "review_clean = re.sub(r\"[^A-zÀ-ÿ0-9' ]+\", \" \", review).lower()\n",
    "review_clean = re.sub(' +', ' ',review_clean).strip()\n",
    "lemmatized = \" \".join([token.lemma_ for token in nlp(review_clean) if token.lemma_ not in STOP_WORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = pd.DataFrame(topic)\n",
    "\n",
    "#####DOUBLE CHECK BINS\n",
    "\n",
    "#Categorizing meta values, the bins are the quantiles used to cut the original dataset (see meta_data_analysis notebook)\n",
    "#length\n",
    "#meta['length'] = pd.cut(meta['len_review'], bins=[   5.,  368., 4998.], labels = ['low', 'high'], include_lowest=True)\n",
    "#uppercase\n",
    "meta['upper_word_count'] = pd.cut(meta['upper_word_count'].rank(method = 'first'), bins=[  0.,   3,   6, 147.], labels = ['low', 'mid', 'high'], include_lowest=True)\n",
    "#exclam\n",
    "meta['exclam_count'] = pd.cut(meta['exclam_count'].rank(method = 'first'), bins=[  0.,   0.01,   2., 133.],labels=['low', 'high', 'very_high'], include_lowest=True)\n",
    "\n",
    "#We concatenate the topic and the metadata \n",
    "conc = pd.concat([topic, meta], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev(text):\n",
    "    #Cleaning\n",
    "    review = text.replace('\\n', ' ')\n",
    "    #Feature Engineering\n",
    "    length = len(review)\n",
    "    exclam_count = len(''.join(ch for ch in review if ch =='!'))\n",
    "    uppercase_word_count = sum(map(str.isupper, review.split()))\n",
    "\n",
    "    meta = pd.DataFrame([[length, exclam_count, uppercase_word_count]], columns = ['len_review', 'exclam_count', 'upper_word_count'])\n",
    "    #Cleaning, Vectorization and Topic Extraction\n",
    "\n",
    "    review_clean = re.sub(r\"[^A-zÀ-ÿ0-9' ]+\", \" \", review).lower()\n",
    "    review_clean = re.sub(' +', ' ',review_clean).strip()\n",
    "    lemmatized = \" \".join([token.lemma_ for token in nlp(review_clean) if token.lemma_ not in STOP_WORDS])\n",
    "\n",
    "    vectrorized_item = text_vectorizer.transform([lemmatized])\n",
    "    topic = topic_extractor.transform(vectrorized_item)\n",
    "\n",
    "    topic = pd.DataFrame(topic)\n",
    "\n",
    "    #####DOUBLE CHECK BINS\n",
    "\n",
    "    #Categorizing meta values, the bins are the quantiles used to cut the original dataset (see meta_data_analysis notebook)\n",
    "    #length\n",
    "    meta['len_review'] = pd.cut(meta['len_review'], bins=[   5.,  368., 4998.], labels = ['low', 'high'], include_lowest=True)\n",
    "    #uppercase\n",
    "    meta['upper_word_count'] = pd.cut(meta['upper_word_count'], bins=[  0.,   1,   2, 147.], labels = ['low', 'mid', 'high'],           include_lowest=True)\n",
    "    #exclam\n",
    "    meta['exclam_count'] = pd.cut(meta['exclam_count'], bins=[  0.,   1,   3., 133.],labels=['low', 'high', 'very_high'], include_lowest=True)\n",
    "\n",
    "    #We concatenate the topic and the metadata \n",
    "    conc = pd.concat([topic, meta], axis = 1)\n",
    "\n",
    "    #preprocessing \n",
    "    data = preprocessor.transform(conc)\n",
    "    prev = lr.predict(data)\n",
    "    proba = lr.predict_proba(data)\n",
    "\n",
    "    return prev[0], proba[0][0], proba[0][1], conc.iloc[-3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'conc' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-84b3c0f1f491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'conc' is not defined"
     ]
    }
   ],
   "source": [
    "conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model_SVD_Text_and_Text_metadata.ipynb\nModel_SVD_Text_and_metadata.ipynb\nModel_Text_and_text_metadata_tensorflow.ipynb\n\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m/\napp.py\ngetprev.py\n\u001b[1m\u001b[36mmodel_files\u001b[m\u001b[m/\n\u001b[1m\u001b[36mtemplates\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getprev import get_prev as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1,\n",
       " 0.10376780326019652,\n",
       " 0.8962321967398035,\n",
       "      0    1    2    3    4    5    6    7    8    9  ...  493  494  495  496  \\\n",
       " 0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       " \n",
       "    497  498  499  len_review  exclam_count  upper_word_count  \n",
       " 0  0.0  0.0  0.0         low     very_high              high  \n",
       " \n",
       " [1 rows x 503 columns])"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "get_prev('ALORS LA CEST TROP !!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             text_review  is_fake_review  \\\n",
       "90594  Très déçu!!!\\nCe soir j'ai eu envie de manger ...               1   \n",
       "90595  J'y vais depuis le début mais j'avoue qu'avec ...               1   \n",
       "90596  Meilleur grec dans les environs + personnel au...               1   \n",
       "\n",
       "                                       text_review_clean  \n",
       "90594  tre decu soir envie manger chicken tika grand ...  \n",
       "90595  y aller debut avoue desormai bon viande faite ...  \n",
       "90596  meilleur grec personnel top superrbe hygiene a...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_review</th>\n      <th>is_fake_review</th>\n      <th>text_review_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>90594</th>\n      <td>Très déçu!!!\\nCe soir j'ai eu envie de manger ...</td>\n      <td>1</td>\n      <td>tre decu soir envie manger chicken tika grand ...</td>\n    </tr>\n    <tr>\n      <th>90595</th>\n      <td>J'y vais depuis le début mais j'avoue qu'avec ...</td>\n      <td>1</td>\n      <td>y aller debut avoue desormai bon viande faite ...</td>\n    </tr>\n    <tr>\n      <th>90596</th>\n      <td>Meilleur grec dans les environs + personnel au...</td>\n      <td>1</td>\n      <td>meilleur grec personnel top superrbe hygiene a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "french_reviews.iloc[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}