{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import jsonfiy from Flask\n",
    "import pandas as pd\n",
    "from flask import Flask, url_for, request, jsonify, json\n",
    "from joblib import dump, load\n",
    "from werkzeug.exceptions import HTTPException\n",
    "\n",
    "\n",
    "main_path = '/Users/admin/Jedha/Fake_reviews_detection/1_Training_models/text_only_models/'\n",
    "#\n",
    "svm = load(main_path + '/main_model.pkl')\n",
    "#\n",
    "preprocessor = load(main_path + '/preprocessor.pkl')\n",
    "#\n",
    "text_vectorizer = load(main_path + '/text_vectorizer.pkl')\n",
    "#\n",
    "topic_extractor = load(main_path + '/topic_extractor.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Bon retour !\\nJe suis revenue dans ce resto après une longue absence de 4 ans. Que dire ? Le chef a changé, la cuisine aussi mais elle reste toujours aussi bonne et fraîche. L'équipe est très jeune et sait préserver cette esprit dynamique sans excès de déconnade. J'ai aimé aussi leur brunch (même trop copieux à mon avis). Bref, j'y retournerai plus souvent !\""
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "review.replace\n",
    "\n",
    "\n",
    "# strip\n",
    "#data[\"text_review_clean\"] = data[\"text_review\"].str.strip()\n",
    "\n",
    "#lower\n",
    "#data[\"text_review_clean\"] = data[\"text_review_clean\"].str.lower()\n",
    "\n",
    "#data[\"text_review_clean\"] = data[\"text_review_clean\"].str.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=Bon retour !\nJe suis revenue dans ce resto après une longue absence de 4 ans. Que dire ? Le chef a changé, la cuisine aussi mais elle reste toujours aussi bonne et fraîche. L'équipe est très jeune et sait préserver cette esprit dynamique sans excès de déconnade. J'ai aimé aussi leur brunch (même trop copieux à mon avis). Bref, j'y retournerai plus souvent !.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0630027fddde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \"\"\"\n\u001b[1;32m    573\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mX_feature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__array__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'allow-nan'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    614\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    617\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=Bon retour !\nJe suis revenue dans ce resto après une longue absence de 4 ans. Que dire ? Le chef a changé, la cuisine aussi mais elle reste toujours aussi bonne et fraîche. L'équipe est très jeune et sait préserver cette esprit dynamique sans excès de déconnade. J'ai aimé aussi leur brunch (même trop copieux à mon avis). Bref, j'y retournerai plus souvent !.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/admin/Jedha\n"
     ]
    }
   ],
   "source": [
    "cd '/Users/admin/Jedha/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import keras.backend as K\n",
    "import pathlib \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import seaborn as sns\n",
    "import io \n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from s3_credentials import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "import unidecode\n",
    "\n",
    "session = boto3.Session(aws_access_key_id= YOUR_ACCESS_KEY, \n",
    "                        aws_secret_access_key= YOUR_SECRET_KEY)\n",
    "\n",
    "s3 = session.resource(\"s3\")\n",
    "client = session.client(\"s3\")\n",
    "\n",
    "# full_dataset_reworked\n",
    "obj = s3.Object('jedha-fake-reviews-project', \"datasets/full_dataset.csv\")\n",
    "dataset = pd.read_csv(io.BytesIO(obj.get()['Body'].read()), low_memory = False, index_col=0)\n",
    "\n",
    "#_____________________________________________________________________\n",
    "######### Cleaning the dataset and adding new columns #########\n",
    "#_____________________________________________________________________\n",
    "\n",
    "#we drop rows in which restaurant infos are not available (miss scraped)\n",
    "dataset = dataset.dropna(subset = ['restaurant_average_rating', 'restaurant_reviews_count', 'restaurant_expensiveness', 'restaurant_name'])\n",
    "\n",
    "#adding a column with the length of the text review\n",
    "dataset['text_length'] = dataset['text_review'].apply(lambda x : len(x))\n",
    "\n",
    "#_____________________________________________________________________\n",
    "######### Fixing existing columns values and types #########\n",
    "#_____________________________________________________________________\n",
    "\n",
    "#for the user_total_image_posted column, if user_total_image_posted is NA it means there is there's no image\n",
    "    # so we set the value to 0\n",
    "dataset.loc[dataset['user_total_image_posted'].isna(), 'user_total_image_posted'] = 0\n",
    "\n",
    "#for the date column,  there is some miss scraps that we want to fix\n",
    "    # a correct data must have a length of 10 , if it is smaller than 10 it's becasue we scrapped the number of images of the user instead\n",
    "    # we may have to scrap again those lines to fix it\n",
    "    # we keep only the rows where the date is correct \n",
    "mask_not_date = dataset['date'].apply(lambda x: len(x)) < 10\n",
    "dataset = dataset.loc[mask_not_date == False, :]\n",
    "    # if te length is greater than 10 is it is beacause we scraped the date + somme additional words ('Avis mis à jour') so we will keep only the part with the date\n",
    "mask_date_to_fix = dataset['date'].apply(lambda x: len(x)) > 10\n",
    "dataset.loc[mask_date_to_fix, 'date' ] = dataset.loc[mask_date_to_fix, 'date' ].str.split('\\n').str[0]\n",
    "    #finally we can convert the date column to a datetime format\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "\n",
    "#for the language column,\n",
    "    # We set all languages other than fr and eu to other to avoiding having lots of categories (causing troubles with OneHotEncoder)\n",
    "mask_is_not_fr_en = (dataset['language'] != 'fr') & (dataset['language'] != 'en')\n",
    "dataset.loc[mask_is_not_fr_en ,'language'] = 'other'\n",
    "\n",
    "#for the username column,\n",
    "    # Setting Not Yelp User = 'Not_Yelp_User\n",
    "dataset.loc[dataset['username'] == 'Not Yelp User', 'username' ] = 'Not_Yelp_User'\n",
    "\n",
    "#for the photos_for_review column, \n",
    "    # value -1 is in fact 0 (no photos found by the scraper)\n",
    "dataset.loc[dataset['photos_for_review'] == '-1.0', 'photos_for_review' ] = '0'\n",
    "    # value L is in fact 0 (no photos found by the scraper but scraped the first letter of \"L'avis du jour\" which happens when the reviews was updated by the user)\n",
    "dataset.loc[dataset['photos_for_review'] == 'L', 'photos_for_review' ] = '0'\n",
    "    # finally we can convert the photos_for_review column to an int format\n",
    "dataset['photos_for_review'] = dataset['photos_for_review'].astype('int')\n",
    "\n",
    "#for the photos_for_review column, \n",
    "    # when there's no info about the expensiveness we set it to -1\n",
    "dataset.loc[dataset['restaurant_expensiveness'] == 'N/C', 'restaurant_expensiveness']  = -1\n",
    "    # we can convert the restaurant_expensiveness column to an int format\n",
    "dataset['restaurant_expensiveness'] = dataset['restaurant_expensiveness'].astype('int')\n",
    "\n",
    "#for the is_real_review column, \n",
    "    # when reverse the values 0 and 1 and rename the column is_fake_review it will make our work easier with sklearn features\n",
    "dataset['is_real_review'] = dataset['is_real_review'].apply(lambda x: 1 if x == 0 else 0)\n",
    "dataset.rename(columns={'is_real_review': 'is_fake_review'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_reviews = dataset.loc[dataset['language'] =='fr',['text_review', 'is_fake_review']].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = french_reviews['text_review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    # Check parameters\n",
    "    if request.json:\n",
    "        # Get JSON as dictionnary\n",
    "        json_input = request.get_json()\n",
    "        value = [[json_input['year']]]\n",
    "        prediction = regressor.predict(value)\n",
    "        \n",
    "        # Return prediction\n",
    "        response = {\n",
    "            # Since prediction is a float and jsonify function can't handle\n",
    "            # floats we need to convert it to string\n",
    "            \"prediction\": str(prediction),\n",
    "        }\n",
    "        return jsonify(response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  }
 ]
}